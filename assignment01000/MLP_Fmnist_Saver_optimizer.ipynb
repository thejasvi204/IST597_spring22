{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnkurMali/IST597_Spring_2022/blob/main/MLP_Fmnist_Saver_optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_iYcla4kCX67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1285f6a3-af53-408b-ff33-5202736231c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# baseline cnn model for fashion mnist\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "seed=5097\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "number_of_train_examples = 60000 #60000\n",
        "number_of_test_examples = 10000   #10000"
      ],
      "metadata": {
        "id": "1YSw24HqXWoy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "JodgHy9nCX68",
        "outputId": "c5717166-7670-4615-ea0e-7252e5b5aad0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#To load the data set for mist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "assert X_train.shape == (60000, 28, 28)\n",
        "assert X_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)\n",
        "\n",
        "\n",
        "# Display randomly selected data\n",
        "indices = list(np.random.randint(X_train.shape[0],size=3))\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.imshow(X_train[indices[i]].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Index {} Class {}\".format(indices[i], y_train[indices[i]]))\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important\n",
        "* Always have a validation set, the procedure to create validation or dev set is by performing random sample without replacement on train set and then only using that fraction as dev set. \n",
        "* Simple approach is to set some K samples, you can extract them from start, mid or end.\n",
        "* Imagine validation set that partially approximates test set distribution and we assume our model would produce identical results when we test it on test set.\n",
        "* Always optimize your hyperparameters by looking at performance on validation set and not test set.\n",
        "* Do not touch test set, we have this to test how our model would work on unseen data."
      ],
      "metadata": {
        "id": "PowjAHuw-wm7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIRI-uLoCX69",
        "outputId": "71de1ccf-60ac-44bb-cb94-15c83f8df7a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of training set is 50000 samples\n",
            "every train example is 28 by 28\n",
            "size of validation set is 10000 samples\n",
            "every validation example is 28 by 28\n",
            "size of training set is 50000 samples\n",
            "every train example has 784 features\n",
            "size of validation set is 10000 samples\n",
            "every validation example has 784 features\n"
          ]
        }
      ],
      "source": [
        "# Split train dataset into train and validation\n",
        "\n",
        "X_val = X_train[50000:60000]\n",
        "X_train = X_train[0:50000]\n",
        "y_val = y_train[50000:60000]\n",
        "y_train = y_train[0:50000]\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example is\", str(X_train.shape[1]), \"by\", str(X_train.shape[2]))\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example is\", str(X_val.shape[1]), \"by\", str(X_val.shape[2]))\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0],-1)#.T\n",
        "X_test = X_test.reshape(X_test.shape[0],-1)\n",
        "X_val = X_val.reshape(X_val.shape[0],-1)\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example has\", str(X_train.shape[1]), \"features\")\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example has\", str(X_val.shape[1]), \"features\")\n",
        "\n",
        "# Split dataset into batches\n",
        "#train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16)\n",
        "#test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Points to remember\n",
        "* If using any type of neural network, normalize your input between 0-1.\n",
        "* One can use various procedures to achieve this, divide by largest value (for images we use 255), subtract mean from data and then normalize, one can even augment them and use other steps for normalization.\n",
        "* Normalization is important step, one could observe significant boost in performance just by having better normalization scheme.\n",
        "* For targets we always use one-hot encodings."
      ],
      "metadata": {
        "id": "VdMEIaFKAscU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDyZ8bZjCX69",
        "outputId": "481cc045-3c0c-4cbc-8a65-1398bc4fb837"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#Normalize Data\n",
        "\n",
        "X_train = X_train/255\n",
        "X_val = X_val/255\n",
        "X_test = X_test/255\n",
        "# X_train[0]\n",
        "np.max(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lIIy313CX69",
        "outputId": "ff27d7d1-b72c-4f3a-d774-581841ff5d11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([10000    10], shape=(2,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "size_input = X_train.shape[1]\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 128\n",
        "size_hidden3 = 128\n",
        "size_output = 10\n",
        "\n",
        "number_of_train_examples = X_train.shape[0]\n",
        "number_of_test_examples = X_test.shape[0]\n",
        "\n",
        "# one hot encode target values\n",
        "y_train = to_categorical(y_train,num_classes=10)\n",
        "y_test = to_categorical(y_test,num_classes=10)\n",
        "y_val = tf.keras.utils.to_categorical(y_val,num_classes=10)\n",
        "\n",
        "print(tf.shape(y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importance of weight initialization\n",
        "\n",
        "* One reason backprop based models can perform bettter lies with the weight initialization method, one important point one should remember is that, if yur weights are initialized to be too high or low, backprop would struggle.\n",
        "* Hence one should always carefully initialize weights of your model, below i have shown approach with random_normal, one can use random_uniform, truncated version of both, Xavier init and orthogonal. \n",
        "* You will find modern day NNs have achieved stable and better performance by simply switching to better init and majority of cases Xavier or Orthogonal works best.\n",
        "* Always initialize your bias using zero or some small constant (ideally 0.01 or less works better). We use bias to shift the activation and in some cases it can stabalize learning, but having large bias can cause negative results.\n",
        "\n",
        "# Loss function\n",
        "\n",
        "* We will always cross-entropy loss for classification.\n",
        "\n",
        "* tf softmax,\n",
        "loss= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf)), this function is simply saying that it will calculate softmax for you, simply provide logits to it. \n",
        "\n",
        "* In other output of your forward pass directly goes this function. Now this operator will calculate or apply softmax over prediction or logits and calculate cross-entropy between prediction and target. I am using reduce_mean since we apply this over batches.\n",
        "* Second is using keras\n",
        "Method 1 :- This function requires logits, hence same as above you will pass logits or output variable to this function. Now remember you need from_logits = True, for this to work.\n",
        "cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "loss_x = cce(y_true_tf, y_pred_tf) \n",
        "\n",
        "* Method 2:- In this we will apply softmax to output function and then pass to CCE loss.\n",
        "So the approach is \n",
        "output = tf.nn.softmax(output)\n",
        "cce = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "loss_x = cce(y_true_tf, y_pred_tf) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Batch normalization post activation model"
      ],
      "metadata": {
        "id": "U7KCVarVCVPW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "obN7WPLpCX69"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    self.gamma=[]\n",
        "    self.beta=[]\n",
        "    self.moving_mean=[]\n",
        "    self.moving_variance=[]\n",
        "    self.sample_mean=[]\n",
        "    self.sample_variance=[]\n",
        "    self.BN_init()\n",
        "    self.momentum = 0.99\n",
        "    self.epsilon=1e-6\n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "  \n",
        " def forward(self, X,training):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X,training)\n",
        "    else:\n",
        "      self.y = self.compute_output(X,training)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf,labels=y_true_tf)\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train,True)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "           \n",
        " def compute_output(self, X,training):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    \n",
        "    #calling batch mean varience function\n",
        "    self.batch_mean_variance(X_tf)\n",
        "\n",
        "    # BN in input layer\n",
        "    if training:\n",
        "      # call BN\n",
        "      X_tf = self.BN(X_tf, self.gamma[0], self.beta[0],self.sample_mean[0], self.sample_variance[0])\n",
        "                \n",
        "      # update the self.moving_mean and self.moving_variance\n",
        "      self.average_assignment( self.moving_mean[0],self.sample_mean[0],self.moving_variance[0], self.sample_variance[0])\n",
        "    else:\n",
        "      # using the updated moving_mean and moving_variance for batch normalization\n",
        "      X_tf = self.BN(X_tf, self.gamma[0], self.beta[0], self.moving_mean[0], self.moving_variance[0])\n",
        "\n",
        "\n",
        "    \n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    \n",
        "    #calling batch mean varience function\n",
        "    self.batch_mean_variance(hhat1)\n",
        "\n",
        "    # BN in hidden layer 1\n",
        "    if training:\n",
        "      # Call BN\n",
        "      hhat1 = self.BN(hhat1, self.gamma[1], self.beta[1], self.sample_mean[1], self.sample_variance[1])\n",
        "                \n",
        "      # update the self.moving_mean1 and self.moving_var1\n",
        "      self.average_assignment(self.moving_mean[1], self.sample_mean[1],self.moving_variance[1], self.sample_variance[1])\n",
        "    else:\n",
        "      # using the updated moving_mean and moving_var for batch normalization\n",
        "      hhat1 = self.BN(hhat1, self.gamma[1], self.beta[1], self.moving_mean[1], self.moving_variance[1])\n",
        "\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    \n",
        "    #calling batch mean varience function\n",
        "    self.batch_mean_variance(hhat2)\n",
        "\n",
        "    # BN in hidden layer 2\n",
        "    if training:\n",
        "      # call BN\n",
        "      hhat2 = self.BN(hhat2, self.gamma[2], self.beta[2], self.sample_mean[2], self.sample_variance[2])\n",
        "                \n",
        "      # update the self.moving_mean2 and self.moving_var2\n",
        "      self.average_assignment(self.moving_mean[2], self.sample_mean[2],self.moving_variance[2], self.sample_variance[2])\n",
        "    else:\n",
        "      # using the updated moving_mean and moving_var for batch normalization\n",
        "      hhat2 = self.BN(hhat2, self.gamma[2], self.beta[2], self.moving_mean[2], self.moving_variance[2])\n",
        "            \n",
        "    what3 = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    hhat3 = tf.nn.relu(what3)\n",
        "    \n",
        "    #calling batch mean varience function\n",
        "    self.batch_mean_variance(hhat3)\n",
        "\n",
        "    # BN in hidden layer 3\n",
        "    if training:\n",
        "      # call BN\n",
        "      hhat3 = self.BN(hhat3, self.gamma[3], self.beta[3], self.sample_mean[3], self.sample_variance[3])\n",
        "                \n",
        "      # update the self.moving_mean3 and self.moving_var3\n",
        "      self.average_assignment(self.moving_mean[3], self.sample_mean[3],self.moving_variance[3], self.sample_variance[3])\n",
        "    else:\n",
        "      # using the updated moving_mean and moving_var for batch normalization\n",
        "      hhat3 = self.BN(hhat3, self.gamma[3], self.beta[3], self.moving_mean[3], self.moving_variance[3])\n",
        "            \n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    return output\n",
        "\n",
        "#Calculating sample mean,varience\n",
        " def batch_mean_variance(self,Z):\n",
        "   self.sample_mean.append(tf.math.reduce_mean(Z))\n",
        "   self.sample_variance.append(tf.math.reduce_variance(Z))\n",
        "\n",
        "#Assigning sample mean,varience to moving mean,varience\n",
        " def average_assignment(self, move_mean, mean_sample,move_variance,variance_sample):\n",
        "        #assigning mean and varience\n",
        "        move_mean.assign(move_mean * self.momentum + mean_sample * (1 - self.momentum))\n",
        "        move_variance.assign(move_variance * self.momentum + variance_sample * (1 - self.momentum))\n",
        "\n",
        " #Batch Normilization\n",
        " def BN(self, x, gamma, beta, moving_mean, moving_varience):\n",
        "    return gamma * (x - moving_mean) / tf.math.sqrt(moving_varience + self.epsilon) + beta\n",
        "\n",
        " #Intialize function for batch normalization\n",
        " def BN_init(self):\n",
        "   #Initialize batch normalization scale parameter gamma\n",
        "   self.gamma = [tf.Variable(tf.ones([1, self.size_input])), tf.Variable(tf.ones([1, self.size_hidden1])), \\\n",
        "                tf.Variable(tf.ones([1, self.size_hidden2])), tf.Variable(tf.ones([1, self.size_hidden3]))]\n",
        "                \n",
        "   # Initialize batch normalization shift parameter beta\n",
        "   self.beta = [tf.Variable(tf.zeros([1, self.size_input])), tf.Variable(tf.zeros([1, self.size_hidden1])),\\\n",
        "               tf.Variable(tf.zeros([1, self.size_hidden2])), tf.Variable(tf.zeros([1, self.size_hidden3]))]\n",
        "\n",
        "   # The moving mean and variance \n",
        "   self.moving_mean = [tf.Variable(tf.zeros([1, self.size_input])), tf.Variable(tf.zeros([1, self.size_hidden1])),\\\n",
        "                       tf.Variable(tf.zeros([1, self.size_hidden2])), tf.Variable(tf.zeros([1, self.size_hidden3]))]\n",
        "\n",
        "   self.moving_variance = [tf.Variable(tf.ones([1, self.size_input])), tf.Variable(tf.ones([1, self.size_hidden1])),\\\n",
        "                           tf.Variable(tf.ones([1, self.size_hidden2])), tf.Variable(tf.ones([1, self.size_hidden3]))]\n",
        "   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Batch Normalization for pre Activation"
      ],
      "metadata": {
        "id": "QQfSGfS1lpnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP_Pre(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    self.gamma=[]\n",
        "    self.beta=[]\n",
        "    self.moving_mean=[]\n",
        "    self.moving_variance=[]\n",
        "    self.sample_mean=[]\n",
        "    self.sample_variance=[]\n",
        "    self.BN_init()\n",
        "    self.momentum = 0.99\n",
        "    self.epsilon=1e-6\n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "  \n",
        " def forward(self, X,training):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X,training)\n",
        "    else:\n",
        "      self.y = self.compute_output(X,training)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf,labels=y_true_tf)\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train,True)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "           \n",
        " def compute_output(self, X,training):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    \n",
        "    #calling batch mean varience function\n",
        "    self.batch_mean_variance(X_tf)\n",
        "\n",
        "    # BN in input layer\n",
        "    if training:\n",
        "      # call BN\n",
        "      X_tf = self.BN(X_tf, self.gamma[0], self.beta[0],self.sample_mean[0], self.sample_variance[0])\n",
        "                \n",
        "      # update the self.moving_mean and self.moving_variance\n",
        "      self.average_assignment( self.moving_mean[0],self.sample_mean[0],self.moving_variance[0], self.sample_variance[0])\n",
        "    else:\n",
        "      # using the updated moving_mean and moving_variance for batch normalization\n",
        "      X_tf = self.BN(X_tf, self.gamma[0], self.beta[0], self.moving_mean[0], self.moving_variance[0])\n",
        "\n",
        "\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    \n",
        "    \n",
        "    #calling batch mean varience function\n",
        "    self.batch_mean_variance(what1)\n",
        "\n",
        "    # BN in hidden layer 1\n",
        "    if training:\n",
        "      # Call BN\n",
        "      what1 = self.BN(what1, self.gamma[1], self.beta[1], self.sample_mean[1], self.sample_variance[1])\n",
        "                \n",
        "      # update the self.moving_mean1 and self.moving_var1\n",
        "      self.average_assignment(self.moving_mean[1], self.sample_mean[1],self.moving_variance[1], self.sample_variance[1])\n",
        "    else:\n",
        "      # using the updated moving_mean and moving_var for batch normalization\n",
        "      what1 = self.BN(what1, self.gamma[1], self.beta[1], self.moving_mean[1], self.moving_variance[1])\n",
        "\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "\n",
        "    \n",
        "    #calling batch mean varience function\n",
        "    self.batch_mean_variance(what2)\n",
        "\n",
        "    # BN in hidden layer 2\n",
        "    if training:\n",
        "      # call BN\n",
        "      what2 = self.BN(what2, self.gamma[2], self.beta[2], self.sample_mean[2], self.sample_variance[2])\n",
        "                \n",
        "      # update the self.moving_mean2 and self.moving_var2\n",
        "      self.average_assignment(self.moving_mean[2], self.sample_mean[2],self.moving_variance[2], self.sample_variance[2])\n",
        "    else:\n",
        "      # using the updated moving_mean and moving_var for batch normalization\n",
        "      what2 = self.BN(what2, self.gamma[2], self.beta[2], self.moving_mean[2], self.moving_variance[2])\n",
        "\n",
        "    hhat2 = tf.nn.relu(what2)     \n",
        "    what3 = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    \n",
        "    #calling batch mean varience function\n",
        "    self.batch_mean_variance(what3)\n",
        "\n",
        "    # BN in hidden layer 3\n",
        "    if training:\n",
        "      # call BN\n",
        "      what3 = self.BN(what3, self.gamma[3], self.beta[3], self.sample_mean[3], self.sample_variance[3])\n",
        "                \n",
        "      # update the self.moving_mean3 and self.moving_var3\n",
        "      self.average_assignment(self.moving_mean[3], self.sample_mean[3],self.moving_variance[3], self.sample_variance[3])\n",
        "    else:\n",
        "      # using the updated moving_mean and moving_var for batch normalization\n",
        "      what3 = self.BN(what3, self.gamma[3], self.beta[3], self.moving_mean[3], self.moving_variance[3])\n",
        "\n",
        "    hhat3 = tf.nn.relu(what3)      \n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat3, self.W4) + self.b4\n",
        "    #output = tf.nn.softmax(output)\n",
        "    return output\n",
        "\n",
        "#Calculating sample mean,varience\n",
        " def batch_mean_variance(self,Z):\n",
        "   self.sample_mean.append(tf.math.reduce_mean(Z))\n",
        "   self.sample_variance.append(tf.math.reduce_variance(Z))\n",
        "\n",
        "#Assigning sample mean,varience to moving mean,varience\n",
        " def average_assignment(self, move_mean, mean_sample,move_variance,variance_sample):\n",
        "        #assigning mean and varience\n",
        "        move_mean.assign(move_mean * self.momentum + mean_sample * (1 - self.momentum))\n",
        "        move_variance.assign(move_variance * self.momentum + variance_sample * (1 - self.momentum))\n",
        "\n",
        "#Batch Normilization\n",
        " def BN(self, x, gamma, beta, moving_mean, moving_varience):\n",
        "    return gamma * (x - moving_mean) / tf.math.sqrt(moving_varience + self.epsilon) + beta\n",
        "\n",
        "#Intialize function for batch normalization\n",
        " def BN_init(self):\n",
        "   #Initialize batch normalization scale parameter gamma\n",
        "   self.gamma = [tf.Variable(tf.ones([1, self.size_input])), tf.Variable(tf.ones([1, self.size_hidden1])), \\\n",
        "                tf.Variable(tf.ones([1, self.size_hidden2])), tf.Variable(tf.ones([1, self.size_hidden3]))]\n",
        "                \n",
        "   # Initialize batch normalization shift parameter beta\n",
        "   self.beta = [tf.Variable(tf.zeros([1, self.size_input])), tf.Variable(tf.zeros([1, self.size_hidden1])),\\\n",
        "               tf.Variable(tf.zeros([1, self.size_hidden2])), tf.Variable(tf.zeros([1, self.size_hidden3]))]\n",
        "\n",
        "   # The moving mean and variance \n",
        "   self.moving_mean = [tf.Variable(tf.zeros([1, self.size_input])), tf.Variable(tf.zeros([1, self.size_hidden1])),\\\n",
        "                       tf.Variable(tf.zeros([1, self.size_hidden2])), tf.Variable(tf.zeros([1, self.size_hidden3]))]\n",
        "\n",
        "   self.moving_variance = [tf.Variable(tf.ones([1, self.size_input])), tf.Variable(tf.ones([1, self.size_hidden1])),\\\n",
        "                           tf.Variable(tf.ones([1, self.size_hidden2])), tf.Variable(tf.ones([1, self.size_hidden3]))]\n",
        "   \n"
      ],
      "metadata": {
        "id": "_q3hU8Nmluk_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train for post activation BN model"
      ],
      "metadata": {
        "id": "g44MdtTlnIl4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOnhvVlUCX6-",
        "outputId": "a3848f55-38eb-4290-dc4c-5405fc357533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Fashion MNIST GPU\n",
            "\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.534360234375 - Validation Accuracy: 84.5800 - Train Accuracy: 85.4740 \n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.408490625 - Validation Accuracy: 85.7900 - Train Accuracy: 87.4940 \n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.375143515625 - Validation Accuracy: 86.3800 - Train Accuracy: 88.3280 \n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.356869453125 - Validation Accuracy: 86.8600 - Train Accuracy: 88.9600 \n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.347125859375 - Validation Accuracy: 86.5300 - Train Accuracy: 89.0880 \n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.3400344921875 - Validation Accuracy: 87.4700 - Train Accuracy: 89.9560 \n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.3375245703125 - Validation Accuracy: 86.5800 - Train Accuracy: 89.1420 \n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.334368828125 - Validation Accuracy: 87.0800 - Train Accuracy: 89.5660 \n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.3340535546875 - Validation Accuracy: 87.1700 - Train Accuracy: 90.1100 \n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.3312072265625 - Validation Accuracy: 86.7000 - Train Accuracy: 89.9860 \n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.329703203125 - Validation Accuracy: 86.6400 - Train Accuracy: 89.9740 \n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.340441875 - Validation Accuracy: 87.0900 - Train Accuracy: 90.5340 \n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.336513828125 - Validation Accuracy: 87.3200 - Train Accuracy: 90.4940 \n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.331766484375 - Validation Accuracy: 85.4400 - Train Accuracy: 88.5480 \n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.3368300390625 - Validation Accuracy: 86.5400 - Train Accuracy: 89.8380 \n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.3451123046875 - Validation Accuracy: 85.9700 - Train Accuracy: 89.8180 \n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.3440289453125 - Validation Accuracy: 85.1900 - Train Accuracy: 89.3740 \n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.3568297265625 - Validation Accuracy: 85.3800 - Train Accuracy: 88.7980 \n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.3614358984375 - Validation Accuracy: 86.2400 - Train Accuracy: 90.1840 \n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.3647074609375 - Validation Accuracy: 85.8700 - Train Accuracy: 89.8580 \n",
            "\n",
            "Total time taken (in seconds): 714.63\n"
          ]
        }
      ],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "mce_fmnist_train=[]\n",
        "mce_fmnist_train_acc=[]\n",
        "mce_fmnist_train_valacc=[]\n",
        "time_start = time.time()\n",
        "print(\"For Fashion MNIST GPU\\n\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(100)\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_gpu.forward(inputs,True) \n",
        "    loss_total = loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "\n",
        "  mce_fmnist_train.append(np.sum(loss_total) / X_train.shape[0])\n",
        "  preds = mlp_on_gpu.forward(X_train,False)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  mce_fmnist_train_acc.append(ds*100)\n",
        "  #print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  #print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_gpu.forward(X_val,False)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "  mce_fmnist_train_valacc.append(cur_val_acc*100)\n",
        "  #print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} - Validation Accuracy: {:.4f} - Train Accuracy: {:.4f} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0],cur_val_acc*100,ds*100))\n",
        "  #plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot learning curve (with errors)\n",
        "\n",
        "trainacc= np.squeeze(mce_fmnist_train_acc)\n",
        "valacc = np.squeeze(mce_fmnist_train_valacc)\n",
        "plt.plot(mce_fmnist_train_acc,label ='Train accuracy')\n",
        "plt.plot(valacc,label ='Validation Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('iterations ')\n",
        "plt.title(\"Fashion MNIST\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "KWdrxXjIktgN",
        "outputId": "98ea9acb-b64a-4f86-fe04-a2497d0071e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train for pre Activation model"
      ],
      "metadata": {
        "id": "-PWGZbxFnOOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_gpu_pre = MLP_Pre(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "mce_fmnist_train_pre=[]\n",
        "mce_fmnist_train_acc_pre=[]\n",
        "mce_fmnist_train_valacc_pre=[]\n",
        "time_start = time.time()\n",
        "print(\"For Fashion MNIST GPU\\n\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(100)\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_gpu_pre.forward(inputs,True) \n",
        "    loss_total = loss_total + mlp_on_gpu_pre.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu_pre.loss(preds, outputs)\n",
        "    mlp_on_gpu_pre.backward(inputs, outputs)\n",
        "\n",
        "  mce_fmnist_train_pre.append(np.sum(loss_total) / X_train.shape[0])\n",
        "  preds = mlp_on_gpu_pre.forward(X_train,False)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  mce_fmnist_train_acc_pre.append(ds*100)\n",
        "  #print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  #print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_gpu_pre.forward(X_val,False)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "  mce_fmnist_train_valacc_pre.append(cur_val_acc*100)\n",
        "  #print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} - Validation Accuracy: {:.4f} - Train Accuracy: {:.4f} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0],cur_val_acc*100,ds*100))\n",
        "  #plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho-nGXb-nTHn",
        "outputId": "59344911-71b5-40b9-d8c0-9d615d4f0f91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Fashion MNIST GPU\n",
            "\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.5371693359375 - Validation Accuracy: 85.1800 - Train Accuracy: 86.0200 \n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.4559314453125 - Validation Accuracy: 83.7400 - Train Accuracy: 84.7560 \n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.547106484375 - Validation Accuracy: 84.1800 - Train Accuracy: 85.1660 \n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.6854546875 - Validation Accuracy: 83.9500 - Train Accuracy: 85.0220 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot learning curve for pre activation(with accuracy)\n",
        "\n",
        "trainacc= np.squeeze(mce_fmnist_train_acc_pre)\n",
        "valacc = np.squeeze(mce_fmnist_train_valacc_pre)\n",
        "plt.plot(mce_fmnist_train_acc,label ='Train accuracy')\n",
        "plt.plot(valacc,label ='Validation Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('iterations ')\n",
        "plt.title(\"Fashion MNIST\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NltJv9sNnpIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting for loss for both pre and post Activation BN models\n",
        "# Plot learning curve (with errors)\n",
        "\n",
        "postmodelerror= np.squeeze(mce_fmnist_train)\n",
        "premodelerror = np.squeeze(mce_fmnist_train_pre)\n",
        "plt.plot(postmodelerror,label ='Post Activation BN')\n",
        "plt.plot(premodelerror,label ='Pre Activation BN')\n",
        "plt.ylabel('Error Losses')\n",
        "plt.xlabel('iterations ')\n",
        "plt.title(\"Fashion MNIST\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5X9w-riKnt73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving your model weights or parameters\n",
        "* It is always advisable to save your model checkpoints every k epochs.\n",
        "* Look at Saver object in tensorflow/keras.\n",
        "* Visualize your model performance using tensorboard.\n",
        "\n",
        "#Steps to save model weights using pickle\n",
        "* Save your model(trainable variables), in our case self.variables into a pickle file.\n",
        "* Load saved file\n",
        "* Redefine model\n",
        "* Load weights\n",
        "* Re-train or test your model"
      ],
      "metadata": {
        "id": "t_wYdNXjhEha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#saving models for BN\n",
        "import pickle\n",
        "models = [MLP, MLP_Pre,]\n",
        "files = [\"MLP_Post.pickle\", \"MLP_Pre.pickle\"]\n",
        "for model, f in zip(models, files):\n",
        "  vars = model.variables\n",
        "  with open(f, 'wb') as handle:\n",
        "    pickle.dump(vars, handle)"
      ],
      "metadata": {
        "id": "-gtwqTXnjW30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading from saved variables\n",
        "with open('MLP_Post.pickle', 'rb') as handle:\n",
        "    b = pickle.load(handle)\n",
        "\n",
        "#Loading from saved variables\n",
        "with open('MLP_Pre.pickle', 'rb') as handle:\n",
        "    b = pickle.load(handle)\n"
      ],
      "metadata": {
        "id": "PvJ9nfrA_c0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "5DRomYsUCX6_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYsXWqI_CX6_"
      },
      "outputs": [],
      "source": [
        "print(\"For Fashion MNIST inference for post Activation BN\\n\")\n",
        "#list to store inferences of test for 10 seeds\n",
        "inference_fmnist=[]\n",
        "\n",
        "##accuracy\n",
        "inference_fmnist_acc=[]\n",
        "\n",
        "Inferences_seeds=[5097,1111,1234,5678,199991,99999,3421,6642,8754,3454444]\n",
        "for seed in Inferences_seeds:\n",
        "  print(\"For seed:{}\\n\".format(seed))\n",
        "  # Initialize\n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "  test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "  #test_loss_total = 0.0\n",
        "  for inputs, outputs in test_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs,True)\n",
        "    test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "  print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "  # Test model\n",
        "  preds_test = mlp_on_gpu.forward(X_test,False)\n",
        "  preds_test = tf.nn.softmax(preds_test)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_test_acc = accuracy.numpy()\n",
        "  inference_fmnist.append(np.sum(test_loss_total.numpy()) / X_test.shape[0])\n",
        "  inference_fmnist_acc.append(cur_test_acc)\n",
        "  print('Test loss: {:.4f} - Test Accuracy:{:.2f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0],cur_test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('weights_pre.pickle', 'rb') as handle:\n",
        "    b = pickle.load(handle)"
      ],
      "metadata": {
        "id": "gilIhv6Jp8Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For Fashion MNIST inference for Pre Activation BN\\n\")\n",
        "#list to store inferences of test for 10 seeds\n",
        "inference_fmnist_pre=[]\n",
        "\n",
        "##accuracy\n",
        "inference_fmnist_acc_pre=[]\n",
        "\n",
        "Inferences_seeds=[5097,1111,1234,5678,199991,99999,3421,6642,8754,3454444]\n",
        "for seed in Inferences_seeds:\n",
        "  print(\"For seed:{}\\n\".format(seed))\n",
        "  # Initialize\n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "  test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "  #test_loss_total = 0.0\n",
        "  for inputs, outputs in test_ds:\n",
        "    preds = mlp_on_gpu_pre.forward(inputs,True)\n",
        "    test_loss_total = test_loss_total + mlp_on_gpu_pre.loss(preds, outputs)\n",
        "  print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "  # Test model\n",
        "  preds_test = mlp_on_gpu_pre.forward(X_test,False)\n",
        "  preds_test = tf.nn.softmax(preds_test)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_test_acc = accuracy.numpy()\n",
        "  inference_fmnist_pre.append(np.sum(test_loss_total.numpy()) / X_test.shape[0])\n",
        "  inference_fmnist_acc_pre.append(cur_test_acc)\n",
        "  print('Test loss: {:.4f} - Test Accuracy:{:.2f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0],cur_test_acc))"
      ],
      "metadata": {
        "id": "lVrAGYlzqBVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot learning curve for pre activation(with error)\n",
        "plt.figure(1)\n",
        "testmodelerror= np.squeeze(inference_fmnist)\n",
        "testpremodelerror = np.squeeze(inference_fmnist_pre)\n",
        "plt.plot(testmodelerror,label ='Test post model error')\n",
        "plt.plot(testpremodelerror,label ='Test pre model error')\n",
        "plt.ylabel('Error')\n",
        "plt.xlabel('iterations ')\n",
        "plt.title(\"Test Fashion MNIST\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# Plot learning curve for pre activation(with error)\n",
        "plt.figure(2)\n",
        "testacc= np.squeeze(inference_fmnist_acc)\n",
        "testpreacc = np.squeeze(inference_fmnist_acc_pre)\n",
        "plt.plot(testacc,label ='Test post BN accuracy')\n",
        "plt.plot(testpreacc,label ='Test Pre BN Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('iterations ')\n",
        "plt.title(\"Test Fashion MNIST\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QrvwNh7gqWKG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "MLP_Fmnist_Saver_optimizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}