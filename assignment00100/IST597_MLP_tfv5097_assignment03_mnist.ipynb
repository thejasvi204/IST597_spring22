{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##* Design MLP with 2 hidden layers (Input Layer - 2 hidden Layer - Output layer) to classify objects digits (MNIST).\n",
        "##* Design Coustomized optimizer and compare with inbuilt optimizer adam.\n",
        "\n",
        "#* tfv5097@psu.edu :Thejasvi Velaga"
      ],
      "metadata": {
        "id": "kkfX8FIJOHgb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPN_DtXq9mns",
        "outputId": "84c4969e-2ecc-4281-add8-caacc7b6fe15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# baseline cnn model for mnist\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "seed=5097\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vtpOvonMQj0f"
      },
      "outputs": [],
      "source": [
        "# Constants \n",
        "size_input = 784\n",
        "size_hidden = [128,128,128]\n",
        "size_output = 10\n",
        "number_of_train_examples = 60000 #60000\n",
        "number_of_test_examples = 10000   #10000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset Function"
      ],
      "metadata": {
        "id": "bxKhx0c7PK2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To load the data set for mist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "eDUuIDRp8Fk2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create validation set\n",
        "X_val = X_train[50000:60000]\n",
        "X_train = X_train[0:50000]\n",
        "y_val = y_train[50000:60000]\n",
        "y_train = y_train[0:50000]"
      ],
      "metadata": {
        "id": "VuftBAkz8Z2a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(X_train.shape[0],-1)#.T\n",
        "X_test = X_test.reshape(X_test.shape[0],-1)\n",
        "X_val = X_val.reshape(X_val.shape[0],-1)"
      ],
      "metadata": {
        "id": "UKKUNhfI8uXK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert from integers to floats\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_val = X_val.astype('float32')\n",
        "# normalize to range 0-1\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "X_val = X_val/255"
      ],
      "metadata": {
        "id": "KePfvxm39fWK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encode target values\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "y_val = tf.keras.utils.to_categorical(y_val)\n"
      ],
      "metadata": {
        "id": "Qh-Ry5HltWrd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_70kdmB6sVbH"
      },
      "source": [
        "##**The below 2 blocks codes is with L1 reguralization and with custom optimization repesctively.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nd7neF1BQpfu"
      },
      "outputs": [],
      "source": [
        "#with L2  regularization.\n",
        "class MLP(tf.keras.Model):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    super(MLP, self).__init__()\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "\n",
        "    # self.size_input = 784\n",
        "    # self.size_hidden = 128\n",
        "    # self.size_output = 10\n",
        "    # self.device = 'gpu'\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer and hideen layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "    # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_hidden[2]]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_hidden[2]]))\n",
        "\n",
        "    # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden[2], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.MLP_variables = [self.W1, self.W2,self.W3,self.W4, self.b1, self.b2, self.b3,self.b4]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf,labels=y_true_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    #optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    #optimizer = tf.keras.optimizers.Adamax(learning_rate=1e-5)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)+0.01*tf.nn.l2_loss(self.W2) +0.001*tf.nn.l2_loss(self.W3)+0.1*tf.nn.l2_loss(self.W4)\n",
        "      #v = tf.concat([tf.reshape(v,[-1]) for v in self.variables[:3]],0)\n",
        "      #current_loss  += 1e-2 * tf.nn.l2_loss(v)\n",
        "    grads = tape.gradient(current_loss, self.MLP_variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.MLP_variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat = tf.nn.relu(what)\n",
        "\n",
        "\n",
        "    what1 = tf.matmul(hhat, self.W2) + self.b2\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "\n",
        "    what2 = tf.matmul(hhat1, self.W3) + self.b3\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W4) + self.b4\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    #output=tf.nn.softmax(output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_cust(tf.keras.Model):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    super(MLP_cust, self).__init__()\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer and hideen layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "    # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_hidden[2]]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_hidden[2]]))\n",
        "\n",
        "    # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden[2], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.MLP_variables = [self.W1, self.W2,self.W3,self.W4, self.b1, self.b2, self.b3,self.b4]\n",
        "\n",
        "    #m,v,u,L,m_corrected,v_corrected,u_corrected=self.initialize_adam()\n",
        "    self.m,self.v,self.u,self.L,self.m_corrected,self.v_corrected,self.u_corrected=[],[],[],4,[],[],[]\n",
        "    \n",
        "    self.t,self.learning_rate,self.beta1, self.beta2, self.beta3,self.epsilon=\\\n",
        "    t,learning_rate,beta1, beta2,beta3,epsilon = 0,0.005,0.9,0.999,0.999999,1e-8\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    return loss_x\n",
        "    #return tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf,labels=y_true_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    #optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    \n",
        "    grads = tape.gradient(current_loss, self.MLP_variables)\n",
        "\n",
        "    lr = 1e-4\n",
        "    dws = grads\n",
        "    dws_new = [i * lr for i in dws]\n",
        "    Wt = [a_i - b_i for a_i, b_i in zip(self.MLP_variables, dws_new)]\n",
        "    self.update_parameters_custopt(grads)\n",
        "    #self.var=var\n",
        "\n",
        "    #for i in range(self.L*2):\n",
        "      #self.MLP_variables[i].assign(Wt[i])\n",
        "  \n",
        "    #print(self.t,\"\\n\")\n",
        "    #return grads\n",
        "    #optimizer.apply_gradients(zip(grads, self.variables))\n",
        "  \n",
        "  def vanillasgd(self, grads, lr = 1e-4):\n",
        "\n",
        "    \"\"\"\n",
        "    Here we will do one-step of sgd to update our weights\n",
        "\n",
        "    \"\"\"\n",
        "    #print(self.MLP_variables[0])\n",
        "    #print(grads[7:8])\n",
        "\n",
        "    self.MLP_variables[0] = self.MLP_variables[0] - tf.math.scalar_mul(lr,dws[0,1])\n",
        "    self.MLP_variables[1] = self.MLP_variables[1] - lr*grads[1,2]\n",
        "    self.MLP_variables[2] = self.MLP_variables[2] - lr*grads[2,3]\n",
        "    self.MLP_variables[3] = self.MLP_variables[3] - lr*grads[3,4]\n",
        "\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat = tf.nn.relu(what)\n",
        "\n",
        "\n",
        "    what1 = tf.matmul(hhat, self.W2) + self.b2\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "\n",
        "    what2 = tf.matmul(hhat1, self.W3) + self.b3\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W4) + self.b4\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    #output=tf.nn.softmax(output)\n",
        "    return output\n",
        "  \n",
        "  def initialize_custopt(self) :\n",
        "    \n",
        "    L = 4 # number of layers in the neural networks\n",
        "    m = []\n",
        "    v = []\n",
        "    u = []\n",
        "    self.t=0\n",
        "    # Initialization of m, v, u. Input: \"self.MLP_variables\". Outputs: \"m, v, u\".\n",
        "    for l in range(L*2):\n",
        "        self.m.append(np.zeros((self.MLP_variables[l]).shape))\n",
        "        self.v.append(np.zeros((self.MLP_variables[l]).shape))\n",
        "        self.u.append(np.zeros((self.MLP_variables[l]).shape))\n",
        "\n",
        "  #update adam \n",
        "  def update_parameters_custopt(self, grads):   \n",
        "    self.t += 1                              \n",
        "    self.grads=grads\n",
        "    self.m_corrected=[]\n",
        "    self.v_corrected=[]\n",
        "    self.u_corrected=[]\n",
        "    wb = []\n",
        "    for l in range(self.L*2):\n",
        "        wb.append(np.zeros((self.MLP_variables[l]).shape))\n",
        "        self.m_corrected.append(np.zeros((self.MLP_variables[l]).shape))\n",
        "        self.v_corrected.append(np.zeros((self.MLP_variables[l]).shape))\n",
        "        self.u_corrected.append(np.zeros((self.MLP_variables[l]).shape))\n",
        "    \n",
        "    #print (self.L)\n",
        "    # Perform Adam update on all parameters\n",
        "    for l in range(self.L*2):\n",
        "      if l<4 :\n",
        "        # Moving average of the squared gradients. Inputs: \"m, grads, beta1\". Output: \"m\".\n",
        "        self.m[l] = self.beta1 * self.m[l] + (1 - self.beta1) * self.grads[l]\n",
        "        #self.m[l+4] = self.beta1 * self.m[l+4] + (1 - self.beta1) * self.grads[l+4]\n",
        "\n",
        "        # Compute bias-corrected first raw moment estimate. Inputs: \"m, beta2, t\". Output: \"m_corrected\".\n",
        "        self.m_corrected[l] = self.m[l] /(1 - self.beta1 ** self.t)\n",
        "        #self.m_corrected[l+4] = self.m[l+4] /(1 - self.beta1 ** self.t)\n",
        "\n",
        "        # Moving average of the gradients. Inputs: \"v, grads, beta2\". Output: \"v\".\n",
        "        self.v[l] = self.beta2 * self.v[l] + (1 - self.beta2) *  (np.square(self.grads[l]) )\n",
        "        #self.v[l+4] = self.beta2 * self.v[l+4] + (1 - self.beta2) *  (np.square(self.grads[l+4]) )\n",
        "\n",
        "        # Compute bias-corrected second moment estimate. Inputs: \"v, beta2, t\". Output: \"v_corrected\".\n",
        "        self.v_corrected[l] = self.v[l] /(1 - self.beta2 ** self.t)\n",
        "        #self.v_corrected[l] = self.v[l+4] /(1 - self.beta2 ** self.t)\n",
        "\n",
        "        # Moving average of the squared gradients. Inputs: \"u, grads, beta3\". Output: \"u\".\n",
        "        self.u[l] = self.beta3 * self.u[l] + (1 - self.beta3) * (self.grads[l]**3 )\n",
        "        #self.u[l+4] = self.beta3 * self.u[l+4] + (1 -self. beta3) * (self.grads[l+4]**3 )\n",
        "\n",
        "        # Compute bias-corrected third raw moment estimate. Inputs: \"u, beta3, t\". Output: \"u_corrected\".\n",
        "        self.u_corrected[l] = self.u[l] /(1 - self.beta3 ** self.t)\n",
        "        #self.u_corrected[l+4] = self.u[l+4] /(1 - self.beta3 ** self.t)\n",
        "\n",
        "        # Update self.MLP_variables. Inputs: \"self.MLP_variables, learning_rate, v_corrected, m_corrected,u_corrected, epsilon\". Output: \"self.MLP_variables\".\n",
        "        #self.MLP_variables[l].assign(self.MLP_variables[l] - self.learning_rate *  self.m_corrected[l] /(self.epsilon+(np.sqrt(np.abs(self.v_corrected[l])) + np.cbrt(np.abs(self.u_corrected[l]))*self.epsilon)))\n",
        "        #self.MLP_variables[l+4].assign(self.MLP_variables[l+4] - self.learning_rate *  self.m_corrected[l+4] / (self.epsilon+(np.sqrt(np.abs(self.v_corrected[l+4])) + np.cbrt(np.abs(self.u_corrected[l+4]))*self.epsilon)))\n",
        "      elif l>=4 and l<8:\n",
        "        # Moving average of the squared gradients. Inputs: \"m, grads, beta1\". Output: \"m\".\n",
        "        #self.m[l] = self.beta1 * self.m[l] + (1 - self.beta1) * self.grads[l]\n",
        "        self.m[l] = self.beta1 * self.m[l] + (1 - self.beta1) * self.grads[l]\n",
        "\n",
        "        # Compute bias-corrected first raw moment estimate. Inputs: \"m, beta2, t\". Output: \"m_corrected\".\n",
        "        #self.m_corrected[l] = self.m[l] /(1 - self.beta1 ** self.t)\n",
        "        self.m_corrected[l] = self.m[l] /(1 - self.beta1 ** self.t)\n",
        "\n",
        "        # Moving average of the gradients. Inputs: \"v, grads, beta2\". Output: \"v\".\n",
        "        #self.v[l] = self.beta2 * self.v[l] + (1 - self.beta2) *  (np.square(self.grads[l]) )\n",
        "        self.v[l] = self.beta2 * self.v[l] + (1 - self.beta2) *  (np.square(self.grads[l]) )\n",
        "\n",
        "        # Compute bias-corrected second moment estimate. Inputs: \"v, beta2, t\". Output: \"v_corrected\".\n",
        "        #self.v_corrected[l] = self.v[l] /(1 - self.beta2 ** self.t)\n",
        "        self.v_corrected[l] = self.v[l] /(1 - self.beta2 ** self.t)\n",
        "\n",
        "        # Moving average of the squared gradients. Inputs: \"u, grads, beta3\". Output: \"u\".\n",
        "        #self.u[l] = self.beta3 * self.u[l] + (1 - self.beta3) * (self.grads[l]**3 )\n",
        "        self.u[l] = self.beta3 * self.u[l] + (1 -self. beta3) * (self.grads[l]**3 )\n",
        "\n",
        "        # Compute bias-corrected third raw moment estimate. Inputs: \"u, beta3, t\". Output: \"u_corrected\".\n",
        "        #self.u_corrected[l] = self.u[l] /(1 - self.beta3 ** self.t)\n",
        "        self.u_corrected[l] = self.u[l] /(1 - self.beta3 ** self.t)\n",
        "\n",
        "      # Update self.MLP_variables. Inputs: \"self.MLP_variables, learning_rate, v_corrected, m_corrected,u_corrected, epsilon\". Output: \"self.MLP_variables\".\n",
        "      #self.MLP_variables[l].assign(self.MLP_variables[l] - self.learning_rate *  self.m_corrected[l] /(self.epsilon+(np.sqrt(np.abs(self.v_corrected[l])) + np.cbrt(np.abs(self.u_corrected[l]))*self.epsilon)))\n",
        "      wb[l] = self.MLP_variables[l]- self.learning_rate *  self.m_corrected[l] / (self.epsilon+(np.sqrt(self.v_corrected[l]) + np.cbrt(self.u_corrected[l])*self.epsilon))\n",
        "\n",
        "    #print (len(wb))\n",
        "    #return self.wb\n",
        "    for i in range(len(self.MLP_variables)):\n",
        "      self.MLP_variables[i].assign(wb[i])\n"
      ],
      "metadata": {
        "id": "s3nOhV6RrPbV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "n9w8SJ9Xf-qT"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS =20\n",
        "iterations=[1,2,3,4,5,6,7,8,9,10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRCVni6FsF3j"
      },
      "source": [
        "##**The below 2 blocks codes is on MNIST with default optimizer and custom optimizer repesctively.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih59_ZCl9D07",
        "outputId": "c7d0ed4a-2b16-425b-ea24-a672b632dc93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For MNIST GPU\n",
            "\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 692.44344 - Validation Accuracy: 80.3500 - Train Accuracy: 79.7140 \n",
            "Number of Epoch = 2 - Average Cross Entropy:= 146.18757 - Validation Accuracy: 85.4000 - Train Accuracy: 85.3640 \n",
            "Number of Epoch = 3 - Average Cross Entropy:= 83.96339 - Validation Accuracy: 87.4100 - Train Accuracy: 87.8120 \n",
            "Number of Epoch = 4 - Average Cross Entropy:= 54.85464 - Validation Accuracy: 88.4500 - Train Accuracy: 89.0880 \n",
            "Number of Epoch = 5 - Average Cross Entropy:= 38.18434 - Validation Accuracy: 88.8000 - Train Accuracy: 89.5440 \n",
            "Number of Epoch = 6 - Average Cross Entropy:= 27.3571525 - Validation Accuracy: 89.6000 - Train Accuracy: 90.1680 \n",
            "Number of Epoch = 7 - Average Cross Entropy:= 20.40848875 - Validation Accuracy: 89.8300 - Train Accuracy: 90.6980 \n",
            "Number of Epoch = 8 - Average Cross Entropy:= 15.17595625 - Validation Accuracy: 90.3600 - Train Accuracy: 90.9620 \n",
            "Number of Epoch = 9 - Average Cross Entropy:= 11.45799125 - Validation Accuracy: 89.9300 - Train Accuracy: 90.8700 \n",
            "Number of Epoch = 10 - Average Cross Entropy:= 8.6464775 - Validation Accuracy: 90.5300 - Train Accuracy: 91.1760 \n",
            "Number of Epoch = 11 - Average Cross Entropy:= 6.584948125 - Validation Accuracy: 90.4200 - Train Accuracy: 91.3000 \n",
            "Number of Epoch = 12 - Average Cross Entropy:= 5.1266353125 - Validation Accuracy: 90.7400 - Train Accuracy: 91.2960 \n",
            "Number of Epoch = 13 - Average Cross Entropy:= 3.8462 - Validation Accuracy: 90.9300 - Train Accuracy: 91.3220 \n",
            "Number of Epoch = 14 - Average Cross Entropy:= 2.906023125 - Validation Accuracy: 90.5000 - Train Accuracy: 90.5540 \n",
            "Number of Epoch = 15 - Average Cross Entropy:= 2.15948921875 - Validation Accuracy: 90.8300 - Train Accuracy: 91.3340 \n",
            "Number of Epoch = 16 - Average Cross Entropy:= 1.54754375 - Validation Accuracy: 90.1200 - Train Accuracy: 90.5940 \n",
            "Number of Epoch = 17 - Average Cross Entropy:= 1.093516484375 - Validation Accuracy: 91.5900 - Train Accuracy: 92.5020 \n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.762530078125 - Validation Accuracy: 91.5700 - Train Accuracy: 92.3120 \n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.5527118359375 - Validation Accuracy: 92.1000 - Train Accuracy: 92.7360 \n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.39376734375 - Validation Accuracy: 92.7200 - Train Accuracy: 93.3440 \n",
            "\n",
            "Total time taken (in seconds): 384.44\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU for mnist with out regularization train set\n",
        "#mlp_on_gpu = MLP()\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "time_start = time.time()\n",
        "mce_mnist_train=[]\n",
        "\n",
        "print(\"For MNIST GPU\\n\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy_train=0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(100)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "    #accuracy_train=tf.keras.metrics.CategoricalAccuracy()\n",
        "    #accuracy_train.update_state(preds, outputs)\n",
        "\n",
        "  mce_mnist_train.append(np.sum(loss_total_gpu) / X_train.shape[0])\n",
        "  # validation and accuracy calculation\n",
        "  preds = mlp_on_gpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  preds_val = mlp_on_gpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} - Validation Accuracy: {:.4f} - Train Accuracy: {:.4f} '.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0],cur_val_acc*100,ds*100))\n",
        "  #plt.plot(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0], 'go')\n",
        "  \n",
        "time_taken = time.time() - time_start\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using GPU for mnist with out regularization train set\n",
        "#mlp_on_gpu = MLP()\n",
        "mlp_on_gpu_cust = MLP_cust(size_input, size_hidden, size_output, device='gpu')\n",
        "time_start = time.time()\n",
        "mce_mnist_train_cust=[]\n",
        "\n",
        "print(\"For MNIST  cust opt GPU\\n\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  mlp_on_gpu_cust.initialize_custopt()\n",
        "  lt = 0\n",
        "  accuracy_train=0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(512)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu_cust.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu_cust.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu_cust.loss(preds, outputs)\n",
        "    mlp_on_gpu_cust.backward(inputs, outputs)\n",
        "    #accuracy_train=tf.keras.metrics.CategoricalAccuracy()\n",
        "    #accuracy_train.update_state(preds, outputs)\n",
        "  mce_mnist_train_cust.append(np.sum(loss_total_gpu) / X_train.shape[0])\n",
        "  # validation and accuracy calculation\n",
        "  preds = mlp_on_gpu_cust.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  preds_val = mlp_on_gpu_cust.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} - Validation Accuracy: {:.4f} - Train Accuracy: {:.4f} '.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0],cur_val_acc*100,ds*100))\n",
        "  #plt.plot(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0], 'go')\n",
        "  \n",
        "time_taken = time.time() - time_start\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "metadata": {
        "id": "ddpfHqIjhzUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db51a178-0a14-44b2-97ee-d686faf28383"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For MNIST  cust opt GPU\n",
            "\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 1.115157734375 - Validation Accuracy: 83.9000 - Train Accuracy: 83.5040 \n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.24750677734375 - Validation Accuracy: 86.0600 - Train Accuracy: 86.7680 \n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.17437345703125 - Validation Accuracy: 87.0200 - Train Accuracy: 88.6180 \n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.13611361328125 - Validation Accuracy: 88.0300 - Train Accuracy: 90.0100 \n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.112529921875 - Validation Accuracy: 88.3400 - Train Accuracy: 90.8020 \n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.09576990234375 - Validation Accuracy: 88.9000 - Train Accuracy: 91.3920 \n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.08328705078125 - Validation Accuracy: 89.1500 - Train Accuracy: 91.9940 \n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0729369384765625 - Validation Accuracy: 89.3500 - Train Accuracy: 92.5020 \n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0647045703125 - Validation Accuracy: 89.5400 - Train Accuracy: 92.9560 \n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0579143505859375 - Validation Accuracy: 89.6100 - Train Accuracy: 93.1480 \n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0517647314453125 - Validation Accuracy: 89.9100 - Train Accuracy: 93.6820 \n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0464691943359375 - Validation Accuracy: 90.0700 - Train Accuracy: 94.0200 \n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0417388427734375 - Validation Accuracy: 90.0100 - Train Accuracy: 94.3420 \n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0377021337890625 - Validation Accuracy: 90.0600 - Train Accuracy: 94.4920 \n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.03404048828125 - Validation Accuracy: 90.2500 - Train Accuracy: 94.8300 \n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.03079493408203125 - Validation Accuracy: 90.3600 - Train Accuracy: 94.9960 \n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.02788538818359375 - Validation Accuracy: 90.3300 - Train Accuracy: 95.2680 \n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.02555072265625 - Validation Accuracy: 90.5100 - Train Accuracy: 95.6280 \n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.02302570068359375 - Validation Accuracy: 90.5100 - Train Accuracy: 95.8700 \n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.02083108154296875 - Validation Accuracy: 90.6300 - Train Accuracy: 95.8680 \n",
            "\n",
            "Total time taken (in seconds): 121.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plotting for MNIST for 2 models"
      ],
      "metadata": {
        "id": "-Dmt9Qu3QI5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot learning curve (with errors)\n",
        "\n",
        "errors= np.squeeze(mce_mnist_train)\n",
        "errors_opt = np.squeeze(mce_mnist_train_cust)\n",
        "plt.plot(errors,label ='MNIST')\n",
        "plt.plot(errors_opt,label ='MNIST custom optimization')\n",
        "plt.ylabel('errors')\n",
        "plt.xlabel('iterations ')\n",
        "plt.title(\"MNIST normal vs custom optimization\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r3aM-WJbQMD0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "c810036d-8dbd-42df-d248-c21dceefe624"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiORnznaqvsU"
      },
      "source": [
        "## One Step Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For MNIST inference\\n\")\n",
        "#list to store inferences of test for 10 seeds\n",
        "inference_mnist=[]\n",
        "\n",
        "##accuracy\n",
        "inference_mnist_acc=[]\n",
        "\n",
        "Inferences_seeds=[5097,1111,1234,5678,199991,99999,3421,6642,8754,3454444]\n",
        "NUM_EPOCHS=10\n",
        "for seed in Inferences_seeds:\n",
        "  print(\"For seed:{}\\n\".format(seed))\n",
        "  mlp_on_gpu_cust = MLP_cust(size_input, size_hidden, size_output, device='gpu')\n",
        "  time_start = time.time()\n",
        "  mce_mnist_train_cust=[]\n",
        "\n",
        "  #print(\"For MNIST  cust opt GPU\\n\")\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "    mlp_on_gpu_cust.initialize_custopt()\n",
        "    lt = 0\n",
        "    accuracy_train=0\n",
        "    accuracy_z = 0.0\n",
        "    cur_train_acc = 0.0\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(512)\n",
        "    for inputs, outputs in train_ds:\n",
        "      preds = mlp_on_gpu_cust.forward(inputs)\n",
        "      loss_total_gpu = loss_total_gpu + mlp_on_gpu_cust.loss(preds, outputs)\n",
        "      lt = lt + mlp_on_gpu_cust.loss(preds, outputs)\n",
        "      mlp_on_gpu_cust.backward(inputs, outputs)\n",
        "      #accuracy_train=tf.keras.metrics.CategoricalAccuracy()\n",
        "      #accuracy_train.update_state(preds, outputs)\n",
        "    mce_mnist_train_cust.append(np.sum(loss_total_gpu) / X_train.shape[0])\n",
        "    # validation and accuracy calculation\n",
        "    preds = mlp_on_gpu_cust.forward(X_train)\n",
        "    # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "    preds = tf.nn.softmax(preds)\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "    accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    cur_train_acc += accuracy_z.numpy()\n",
        "    ds = cur_train_acc\n",
        "    preds_val = mlp_on_gpu_cust.forward(X_val)\n",
        "    preds_val = tf.nn.softmax(preds_val)\n",
        "    correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    cur_val_acc = accuracy.numpy()\n",
        "\n",
        "    #print('Number of Epoch = {} - Average Cross Entropy:= {} - Validation Accuracy: {:.4f} - Train Accuracy: {:.4f} '.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0],cur_val_acc*100,ds*100))\n",
        "  \n",
        "  time_taken = time.time() - time_start\n",
        "  #print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "\n",
        "\n",
        "  # Test DS for MNIST \n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "  test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(100)\n",
        "\n",
        "  #test_loss_total = 0.0\n",
        "  for inputs, outputs in test_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "\n",
        "  # Test model\n",
        "  preds_test = mlp_on_gpu.forward(X_test)\n",
        "  preds_test = tf.nn.softmax(preds_test)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_test_acc = accuracy.numpy()\n",
        "  inference_mnist.append(np.sum(test_loss_total.numpy()) / X_test.shape[0])\n",
        "  inference_mnist_acc.append(cur_test_acc)\n",
        "  print('Test loss: {:.4f} - Test Accuracy:{:.2f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0],cur_test_acc))\n",
        "  "
      ],
      "metadata": {
        "id": "v50dIkY6RaNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35b99ee0-89a3-48e2-a416-8fcde427a9ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For MNIST inference\n",
            "\n",
            "For seed:5097\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For MNIST custom inference\\n\")\n",
        "#list to store inferences of test for 10 seeds\n",
        "inference_mnist_cust=[]\n",
        "\n",
        "##accuracy\n",
        "inference_mnist_cust_acc=[]\n",
        "\n",
        "Inferences_seeds=[5097,1111,1234,5678,199991,99999,3421,6642,8754,3454444]\n",
        "\n",
        "for seed in Inferences_seeds:\n",
        "  print(\"For seed:{}\\n\".format(seed))\n",
        "  mlp_on_gpu_cust = MLP_cust(size_input, size_hidden, size_output, device='gpu')\n",
        "  time_start = time.time()\n",
        "  mce_mnist_train=[]\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "    mlp_on_gpu_cust.initialize_custopt()\n",
        "    lt = 0\n",
        "    accuracy_train=0\n",
        "    accuracy_z = 0.0\n",
        "    cur_train_acc = 0.0\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(512)\n",
        "    for inputs, outputs in train_ds:\n",
        "      preds = mlp_on_gpu_cust.forward(inputs)\n",
        "      loss_total_gpu = loss_total_gpu + mlp_on_gpu_cust.loss(preds, outputs)\n",
        "      lt = lt + mlp_on_gpu_cust.loss(preds, outputs)\n",
        "      mlp_on_gpu_cust.backward(inputs, outputs)\n",
        "    #accuracy_train=tf.keras.metrics.CategoricalAccuracy()\n",
        "    #accuracy_train.update_state(preds, outputs)\n",
        "    mce_mnist_train_cust.append(np.sum(loss_total_gpu) / X_train.shape[0])\n",
        "    # validation and accuracy calculation\n",
        "    preds = mlp_on_gpu_cust.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  preds_val = mlp_on_gpu_cust.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "    #print('Number of Epoch = {} - Average Cross Entropy:= {} - Validation Accuracy: {:.4f} - Train Accuracy: {:.4f} '.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0],cur_val_acc*100,ds*100))\n",
        "  \n",
        "  time_taken = time.time() - time_start\n",
        "  #print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "\n",
        "\n",
        "  # Test DS for MNIST \n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "  test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(100)\n",
        "\n",
        "  #test_loss_total = 0.0\n",
        "  for inputs, outputs in test_ds:\n",
        "    preds = mlp_on_gpu_cust.forward(inputs)\n",
        "    test_loss_total = test_loss_total + mlp_on_gpu_cust.loss(preds, outputs)\n",
        "\n",
        "  # Test model\n",
        "  preds_test = mlp_on_gpu_cust.forward(X_test)\n",
        "  preds_test = tf.nn.softmax(preds_test)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_test_acc = accuracy.numpy()\n",
        "  inference_mnist_cust.append(np.sum(test_loss_total.numpy()) / X_test.shape[0])\n",
        "  inference_mnist_cust_acc.append(cur_test_acc)\n",
        "  print('Test loss: {:.4f} - Test Accuracy:{:.2f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0],cur_test_acc))\n",
        "  "
      ],
      "metadata": {
        "id": "6pxm_MybL_xu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "outputId": "5bbf2fae-cce7-41a0-eb7a-5f5fa9d04cdc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For MNIST custom inference\n",
            "\n",
            "For seed:5097\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ba148aea1b49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mloss_total_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_total_gpu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmlp_on_gpu_cust\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mlt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmlp_on_gpu_cust\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mmlp_on_gpu_cust\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m#accuracy_train=tf.keras.metrics.CategoricalAccuracy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#accuracy_train.update_state(preds, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-bb873eaba1ae>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mdws_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdws\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mWt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma_i\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb_i\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMLP_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdws_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameters_custopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;31m#self.var=var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-bb873eaba1ae>\u001b[0m in \u001b[0;36mupdate_parameters_custopt\u001b[0;34m(self, grads)\u001b[0m\n\u001b[1;32m    161\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# Moving average of the squared gradients. Inputs: \"m, grads, beta1\". Output: \"m\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;31m#self.m[l+4] = self.beta1 * self.m[l+4] + (1 - self.beta1) * self.grads[l+4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/data_structures.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getslice__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot learning curve (with errors)\n",
        "\n",
        "errors= np.squeeze(inference_mnist)\n",
        "errors_opt = np.squeeze(inference_mnist_cust)\n",
        "plt.plot(errors,label ='Test MNIST')\n",
        "plt.plot(errors_opt,label ='Test MNIST custom optimization')\n",
        "plt.ylabel('errors')\n",
        "plt.xlabel('iterations ')\n",
        "plt.title(\"MNIST normal vs custom optimization\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r7ATpiTl85da"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "IST597_MLP_tfv5097_assignment03_mnist.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}