{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##* Design MLP with 2 hidden layers (Input Layer - 2 hidden Layer - Output layer) to classify objects digits (Fashon MNIST).\n",
        "##* Design Coustomized optimizer and compare with inbuilt optimizer adam.\n",
        "\n",
        "#* tfv5097@psu.edu :Thejasvi Velaga"
      ],
      "metadata": {
        "id": "kkfX8FIJOHgb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPN_DtXq9mns",
        "outputId": "a16417fe-7782-4b99-916e-f8fe0bfb5f23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# baseline cnn model for fashion mnist\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "seed=5097\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtpOvonMQj0f"
      },
      "outputs": [],
      "source": [
        "# Constants \n",
        "size_input = 784\n",
        "size_hidden = [128,128,128]\n",
        "size_output = 10\n",
        "number_of_train_examples = 60000 #60000\n",
        "number_of_test_examples = 10000   #10000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset Function"
      ],
      "metadata": {
        "id": "bxKhx0c7PK2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To load the data set for mist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "eDUuIDRp8Fk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create validation set\n",
        "X_val = X_train[50000:60000]\n",
        "X_train = X_train[0:50000]\n",
        "y_val = y_train[50000:60000]\n",
        "y_train = y_train[0:50000]"
      ],
      "metadata": {
        "id": "VuftBAkz8Z2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(X_train.shape[0],-1)#.T\n",
        "X_test = X_test.reshape(X_test.shape[0],-1)\n",
        "X_val = X_val.reshape(X_val.shape[0],-1)"
      ],
      "metadata": {
        "id": "UKKUNhfI8uXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert from integers to floats\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_val = X_val.astype('float32')\n",
        "# normalize to range 0-1\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "X_val = X_val/255"
      ],
      "metadata": {
        "id": "KePfvxm39fWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encode target values\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "y_val = tf.keras.utils.to_categorical(y_val)\n"
      ],
      "metadata": {
        "id": "Qh-Ry5HltWrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_70kdmB6sVbH"
      },
      "source": [
        "##**The below 2 blocks codes is with L1 reguralization and with custom optimization repesctively.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd7neF1BQpfu"
      },
      "outputs": [],
      "source": [
        "#with L2  regularization.\n",
        "class MLP(tf.keras.Model):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    super(MLP, self).__init__()\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "\n",
        "    # self.size_input = 784\n",
        "    # self.size_hidden = 128\n",
        "    # self.size_output = 10\n",
        "    # self.device = 'gpu'\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer and hideen layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "    # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_hidden[2]]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_hidden[2]]))\n",
        "\n",
        "    # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden[2], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.MLP_variables = [self.W1, self.W2,self.W3,self.W4, self.b1, self.b2, self.b3,self.b4]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf,labels=y_true_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    #optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    #optimizer = tf.keras.optimizers.Adamax(learning_rate=1e-5)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)+0.01*tf.nn.l2_loss(self.W2) +0.001*tf.nn.l2_loss(self.W3)+0.1*tf.nn.l2_loss(self.W4)\n",
        "      #v = tf.concat([tf.reshape(v,[-1]) for v in self.variables[:3]],0)\n",
        "      #current_loss  += 1e-2 * tf.nn.l2_loss(v)\n",
        "    grads = tape.gradient(current_loss, self.MLP_variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.MLP_variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat = tf.nn.relu(what)\n",
        "\n",
        "\n",
        "    what1 = tf.matmul(hhat, self.W2) + self.b2\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "\n",
        "    what2 = tf.matmul(hhat1, self.W3) + self.b3\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W4) + self.b4\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    #output=tf.nn.softmax(output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_cust(tf.keras.Model):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    super(MLP_cust, self).__init__()\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer and hideen layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "    # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_hidden[2]]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_hidden[2]]))\n",
        "\n",
        "    # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden[2], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.MLP_variables = [self.W1, self.W2,self.W3,self.W4, self.b1, self.b2, self.b3,self.b4]\n",
        "\n",
        "    #m,v,u,L,m_corrected,v_corrected,u_corrected=self.initialize_adam()\n",
        "    self.m,self.v,self.u,self.L,self.m_corrected,self.v_corrected,self.u_corrected=[],[],[],4,[],[],[]\n",
        "    \n",
        "    self.t,self.learning_rate,self.beta1, self.beta2, self.beta3,self.epsilon=\\\n",
        "    t,learning_rate,beta1, beta2,beta3,epsilon = 0,0.005,0.9,0.999,0.999999,1e-8\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    return loss_x\n",
        "    #return tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf,labels=y_true_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    #optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    \n",
        "    grads = tape.gradient(current_loss, self.MLP_variables)\n",
        "\n",
        "    lr = 1e-4\n",
        "    dws = grads\n",
        "    dws_new = [i * lr for i in dws]\n",
        "    Wt = [a_i - b_i for a_i, b_i in zip(self.MLP_variables, dws_new)]\n",
        "    self.update_parameters_custopt(grads)\n",
        "    #self.var=var\n",
        "\n",
        "    #for i in range(self.L*2):\n",
        "      #self.MLP_variables[i].assign(Wt[i])\n",
        "  \n",
        "    #print(self.t,\"\\n\")\n",
        "    #return grads\n",
        "    #optimizer.apply_gradients(zip(grads, self.variables))\n",
        "  \n",
        "  def vanillasgd(self, grads, lr = 1e-4):\n",
        "\n",
        "    \"\"\"\n",
        "    Here we will do one-step of sgd to update our weights\n",
        "\n",
        "    \"\"\"\n",
        "    #print(self.MLP_variables[0])\n",
        "    #print(grads[7:8])\n",
        "\n",
        "    self.MLP_variables[0] = self.MLP_variables[0] - tf.math.scalar_mul(lr,dws[0,1])\n",
        "    self.MLP_variables[1] = self.MLP_variables[1] - lr*grads[1,2]\n",
        "    self.MLP_variables[2] = self.MLP_variables[2] - lr*grads[2,3]\n",
        "    self.MLP_variables[3] = self.MLP_variables[3] - lr*grads[3,4]\n",
        "\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    # Compute values in hidden layer\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat = tf.nn.relu(what)\n",
        "\n",
        "\n",
        "    what1 = tf.matmul(hhat, self.W2) + self.b2\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "\n",
        "    what2 = tf.matmul(hhat1, self.W3) + self.b3\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W4) + self.b4\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    #output=tf.nn.softmax(output)\n",
        "    return output\n",
        "  \n",
        "  def initialize_custopt(self) :\n",
        "    \n",
        "    L = 4 # number of layers in the neural networks\n",
        "    m = []\n",
        "    v = []\n",
        "    u = []\n",
        "    self.t=0\n",
        "    # Initialization of m, v, u. Input: \"self.MLP_variables\". Outputs: \"m, v, u\".\n",
        "    for l in range(L*2):\n",
        "        self.m.append(np.zeros((self.MLP_variables[l]).shape))\n",
        "        self.v.append(np.zeros((self.MLP_variables[l]).shape))\n",
        "        self.u.append(np.zeros((self.MLP_variables[l]).shape))\n",
        "\n",
        "  #update adam \n",
        "  def update_parameters_custopt(self, grads):   \n",
        "    self.t += 1                              \n",
        "    self.grads=grads\n",
        "    self.m_corrected=[]\n",
        "    self.v_corrected=[]\n",
        "    self.u_corrected=[]\n",
        "    wb = []\n",
        "    for l in range(self.L*2):\n",
        "        wb.append(np.zeros((self.MLP_variables[l]).shape))\n",
        "        self.m_corrected.append(np.zeros((self.MLP_variables[l]).shape))\n",
        "        self.v_corrected.append(np.zeros((self.MLP_variables[l]).shape))\n",
        "        self.u_corrected.append(np.zeros((self.MLP_variables[l]).shape))\n",
        "    \n",
        "    #print (self.L)\n",
        "    # Perform Adam update on all parameters\n",
        "    for l in range(self.L*2):\n",
        "      if l<4 :\n",
        "        # Moving average of the squared gradients. Inputs: \"m, grads, beta1\". Output: \"m\".\n",
        "        self.m[l] = self.beta1 * self.m[l] + (1 - self.beta1) * self.grads[l]\n",
        "        #self.m[l+4] = self.beta1 * self.m[l+4] + (1 - self.beta1) * self.grads[l+4]\n",
        "\n",
        "        # Compute bias-corrected first raw moment estimate. Inputs: \"m, beta2, t\". Output: \"m_corrected\".\n",
        "        self.m_corrected[l] = self.m[l] /(1 - self.beta1 ** self.t)\n",
        "        #self.m_corrected[l+4] = self.m[l+4] /(1 - self.beta1 ** self.t)\n",
        "\n",
        "        # Moving average of the gradients. Inputs: \"v, grads, beta2\". Output: \"v\".\n",
        "        self.v[l] = self.beta2 * self.v[l] + (1 - self.beta2) *  (np.square(self.grads[l]) )\n",
        "        #self.v[l+4] = self.beta2 * self.v[l+4] + (1 - self.beta2) *  (np.square(self.grads[l+4]) )\n",
        "\n",
        "        # Compute bias-corrected second moment estimate. Inputs: \"v, beta2, t\". Output: \"v_corrected\".\n",
        "        self.v_corrected[l] = self.v[l] /(1 - self.beta2 ** self.t)\n",
        "        #self.v_corrected[l] = self.v[l+4] /(1 - self.beta2 ** self.t)\n",
        "\n",
        "        # Moving average of the squared gradients. Inputs: \"u, grads, beta3\". Output: \"u\".\n",
        "        self.u[l] = self.beta3 * self.u[l] + (1 - self.beta3) * (self.grads[l]**3 )\n",
        "        #self.u[l+4] = self.beta3 * self.u[l+4] + (1 -self. beta3) * (self.grads[l+4]**3 )\n",
        "\n",
        "        # Compute bias-corrected third raw moment estimate. Inputs: \"u, beta3, t\". Output: \"u_corrected\".\n",
        "        self.u_corrected[l] = self.u[l] /(1 - self.beta3 ** self.t)\n",
        "        #self.u_corrected[l+4] = self.u[l+4] /(1 - self.beta3 ** self.t)\n",
        "\n",
        "        # Update self.MLP_variables. Inputs: \"self.MLP_variables, learning_rate, v_corrected, m_corrected,u_corrected, epsilon\". Output: \"self.MLP_variables\".\n",
        "        #self.MLP_variables[l].assign(self.MLP_variables[l] - self.learning_rate *  self.m_corrected[l] /(self.epsilon+(np.sqrt(np.abs(self.v_corrected[l])) + np.cbrt(np.abs(self.u_corrected[l]))*self.epsilon)))\n",
        "        #self.MLP_variables[l+4].assign(self.MLP_variables[l+4] - self.learning_rate *  self.m_corrected[l+4] / (self.epsilon+(np.sqrt(np.abs(self.v_corrected[l+4])) + np.cbrt(np.abs(self.u_corrected[l+4]))*self.epsilon)))\n",
        "      elif l>=4 and l<8:\n",
        "        # Moving average of the squared gradients. Inputs: \"m, grads, beta1\". Output: \"m\".\n",
        "        #self.m[l] = self.beta1 * self.m[l] + (1 - self.beta1) * self.grads[l]\n",
        "        self.m[l] = self.beta1 * self.m[l] + (1 - self.beta1) * self.grads[l]\n",
        "\n",
        "        # Compute bias-corrected first raw moment estimate. Inputs: \"m, beta2, t\". Output: \"m_corrected\".\n",
        "        #self.m_corrected[l] = self.m[l] /(1 - self.beta1 ** self.t)\n",
        "        self.m_corrected[l] = self.m[l] /(1 - self.beta1 ** self.t)\n",
        "\n",
        "        # Moving average of the gradients. Inputs: \"v, grads, beta2\". Output: \"v\".\n",
        "        #self.v[l] = self.beta2 * self.v[l] + (1 - self.beta2) *  (np.square(self.grads[l]) )\n",
        "        self.v[l] = self.beta2 * self.v[l] + (1 - self.beta2) *  (np.square(self.grads[l]) )\n",
        "\n",
        "        # Compute bias-corrected second moment estimate. Inputs: \"v, beta2, t\". Output: \"v_corrected\".\n",
        "        #self.v_corrected[l] = self.v[l] /(1 - self.beta2 ** self.t)\n",
        "        self.v_corrected[l] = self.v[l] /(1 - self.beta2 ** self.t)\n",
        "\n",
        "        # Moving average of the squared gradients. Inputs: \"u, grads, beta3\". Output: \"u\".\n",
        "        #self.u[l] = self.beta3 * self.u[l] + (1 - self.beta3) * (self.grads[l]**3 )\n",
        "        self.u[l] = self.beta3 * self.u[l] + (1 -self. beta3) * (self.grads[l]**3 )\n",
        "\n",
        "        # Compute bias-corrected third raw moment estimate. Inputs: \"u, beta3, t\". Output: \"u_corrected\".\n",
        "        #self.u_corrected[l] = self.u[l] /(1 - self.beta3 ** self.t)\n",
        "        self.u_corrected[l] = self.u[l] /(1 - self.beta3 ** self.t)\n",
        "\n",
        "      # Update self.MLP_variables. Inputs: \"self.MLP_variables, learning_rate, v_corrected, m_corrected,u_corrected, epsilon\". Output: \"self.MLP_variables\".\n",
        "      #self.MLP_variables[l].assign(self.MLP_variables[l] - self.learning_rate *  self.m_corrected[l] /(self.epsilon+(np.sqrt(np.abs(self.v_corrected[l])) + np.cbrt(np.abs(self.u_corrected[l]))*self.epsilon)))\n",
        "      wb[l] = self.MLP_variables[l]- self.learning_rate *  self.m_corrected[l] / (self.epsilon+(np.sqrt(self.v_corrected[l]) + np.cbrt(self.u_corrected[l])*self.epsilon))\n",
        "\n",
        "    #print (len(wb))\n",
        "    #return self.wb\n",
        "    for i in range(len(self.MLP_variables)):\n",
        "      self.MLP_variables[i].assign(wb[i])\n"
      ],
      "metadata": {
        "id": "s3nOhV6RrPbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9w8SJ9Xf-qT"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS =20\n",
        "iterations=[1,2,3,4,5,6,7,8,9,10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRCVni6FsF3j"
      },
      "source": [
        "##**The below 2 blocks codes is on MNIST with default optimizer and custom optimizer repesctively.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih59_ZCl9D07",
        "outputId": "f29872dd-1348-4edf-82c5-64a5107db7d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For MNIST GPU\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU for mnist with out regularization train set\n",
        "#mlp_on_gpu = MLP()\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "time_start = time.time()\n",
        "mce_mnist_train=[]\n",
        "\n",
        "print(\"For Fashion MNIST GPU\\n\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy_train=0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(100)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "    #accuracy_train=tf.keras.metrics.CategoricalAccuracy()\n",
        "    #accuracy_train.update_state(preds, outputs)\n",
        "\n",
        "  mce_mnist_train.append(np.sum(loss_total_gpu) / X_train.shape[0])\n",
        "  # validation and accuracy calculation\n",
        "  preds = mlp_on_gpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  preds_val = mlp_on_gpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} - Validation Accuracy: {:.4f} - Train Accuracy: {:.4f} '.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0],cur_val_acc*100,ds*100))\n",
        "  #plt.plot(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0], 'go')\n",
        "  \n",
        "time_taken = time.time() - time_start\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model using GPU for mnist with out regularization train set\n",
        "#mlp_on_gpu = MLP()\n",
        "mlp_on_gpu_cust = MLP_cust(size_input, size_hidden, size_output, device='gpu')\n",
        "time_start = time.time()\n",
        "mce_mnist_train_cust=[]\n",
        "\n",
        "print(\"For Fashion MNIST  cust opt GPU\\n\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  mlp_on_gpu_cust.initialize_custopt()\n",
        "  lt = 0\n",
        "  accuracy_train=0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(512)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu_cust.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu_cust.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu_cust.loss(preds, outputs)\n",
        "    mlp_on_gpu_cust.backward(inputs, outputs)\n",
        "    #accuracy_train=tf.keras.metrics.CategoricalAccuracy()\n",
        "    #accuracy_train.update_state(preds, outputs)\n",
        "  mce_mnist_train_cust.append(np.sum(loss_total_gpu) / X_train.shape[0])\n",
        "  # validation and accuracy calculation\n",
        "  preds = mlp_on_gpu_cust.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  preds_val = mlp_on_gpu_cust.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} - Validation Accuracy: {:.4f} - Train Accuracy: {:.4f} '.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0],cur_val_acc*100,ds*100))\n",
        "  #plt.plot(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0], 'go')\n",
        "  \n",
        "time_taken = time.time() - time_start\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "metadata": {
        "id": "ddpfHqIjhzUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plotting for MNIST for 2 models"
      ],
      "metadata": {
        "id": "-Dmt9Qu3QI5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot learning curve (with errors)\n",
        "\n",
        "errors= np.squeeze(mce_mnist_train)\n",
        "errors_opt = np.squeeze(mce_mnist_train_cust)\n",
        "plt.plot(iterations,errors,label ='Fashion MNIST')\n",
        "plt.plot(iterations,errors_opt,label ='Fashion MNIST custom optimization')\n",
        "plt.ylabel('errors')\n",
        "plt.xlabel('iterations ')\n",
        "plt.title(\"Fashion MNIST normal vs custom optimization\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r3aM-WJbQMD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiORnznaqvsU"
      },
      "source": [
        "## One Step Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For Fashion MNIST inference\\n\")\n",
        "#list to store inferences of test for 10 seeds\n",
        "inference_mnist=[]\n",
        "\n",
        "##accuracy\n",
        "inference_mnist_acc=[]\n",
        "\n",
        "Inferences_seeds=[5097,1111,1234,5678,199991,99999,3421,6642,8754,3454444]\n",
        "NUM_EPOCHS=10\n",
        "for seed in Inferences_seeds:\n",
        "  print(\"For seed:{}\\n\".format(seed))\n",
        "  mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "  time_start = time.time()\n",
        "  mce_mnist_train=[]\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "    lt = 0\n",
        "    accuracy_train=0\n",
        "    accuracy_z = 0.0\n",
        "    cur_train_acc = 0.0\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(100)\n",
        "    for inputs, outputs in train_ds:\n",
        "      preds = mlp_on_gpu.forward(inputs)\n",
        "      loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "      lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "      mlp_on_gpu.backward(inputs, outputs)\n",
        "    #accuracy_train=tf.keras.metrics.CategoricalAccuracy()\n",
        "    #accuracy_train.update_state(preds, outputs)\n",
        "\n",
        "    mce_mnist_train.append(np.sum(loss_total_gpu) / X_train.shape[0])\n",
        "    # validation and accuracy calculation\n",
        "    preds = mlp_on_gpu.forward(X_train)\n",
        "    # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "    preds = tf.nn.softmax(preds)\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "    accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    cur_train_acc += accuracy_z.numpy()\n",
        "    ds = cur_train_acc\n",
        "    preds_val = mlp_on_gpu.forward(X_val)\n",
        "    preds_val = tf.nn.softmax(preds_val)\n",
        "    correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    cur_val_acc = accuracy.numpy()\n",
        "\n",
        "    #print('Number of Epoch = {} - Average Cross Entropy:= {} - Validation Accuracy: {:.4f} - Train Accuracy: {:.4f} '.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0],cur_val_acc*100,ds*100))\n",
        "  \n",
        "  time_taken = time.time() - time_start\n",
        "  #print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "\n",
        "\n",
        "  # Test DS for MNIST \n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "  test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(100)\n",
        "\n",
        "  #test_loss_total = 0.0\n",
        "  for inputs, outputs in test_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "\n",
        "  # Test model\n",
        "  preds_test = mlp_on_gpu.forward(X_test)\n",
        "  preds_test = tf.nn.softmax(preds_test)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_test_acc = accuracy.numpy()\n",
        "  inference_mnist.append(np.sum(test_loss_total.numpy()) / X_test.shape[0])\n",
        "  inference_mnist_acc.append(cur_test_acc)\n",
        "  print('Test loss: {:.4f} - Test Accuracy:{:.2f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0],cur_test_acc))\n",
        "  "
      ],
      "metadata": {
        "id": "v50dIkY6RaNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For Fashion MNIST custom inference\\n\")\n",
        "#list to store inferences of test for 10 seeds\n",
        "inference_mnist_cust=[]\n",
        "\n",
        "##accuracy\n",
        "inference_mnist_cust_acc=[]\n",
        "\n",
        "Inferences_seeds=[5097,1111,1234,5678,199991,99999,3421,6642,8754,3454444]\n",
        "\n",
        "for seed in Inferences_seeds:\n",
        "  print(\"For seed:{}\\n\".format(seed))\n",
        "  mlp_on_gpu_cust = MLP_cust(size_input, size_hidden, size_output, device='gpu')\n",
        "time_start = time.time()\n",
        "mce_mnist_train_cust=[]\n",
        "\n",
        "print(\"For Fashion MNIST  cust opt GPU\\n\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  mlp_on_gpu_cust.initialize_custopt()\n",
        "  lt = 0\n",
        "  accuracy_train=0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(512)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu_cust.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu_cust.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu_cust.loss(preds, outputs)\n",
        "    mlp_on_gpu_cust.backward(inputs, outputs)\n",
        "    #accuracy_train=tf.keras.metrics.CategoricalAccuracy()\n",
        "    #accuracy_train.update_state(preds, outputs)\n",
        "  mce_mnist_train_cust.append(np.sum(loss_total_gpu) / X_train.shape[0])\n",
        "  # validation and accuracy calculation\n",
        "  preds = mlp_on_gpu_cust.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  preds_val = mlp_on_gpu_cust.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "    #print('Number of Epoch = {} - Average Cross Entropy:= {} - Validation Accuracy: {:.4f} - Train Accuracy: {:.4f} '.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0],cur_val_acc*100,ds*100))\n",
        "  \n",
        "  time_taken = time.time() - time_start\n",
        "  #print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "\n",
        "\n",
        "  # Test DS for MNIST \n",
        "  test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "  correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "  test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(100)\n",
        "\n",
        "  #test_loss_total = 0.0\n",
        "  for inputs, outputs in test_ds:\n",
        "    preds = mlp_on_gpu_cust.forward(inputs)\n",
        "    test_loss_total = test_loss_total + mlp_on_gpu_cust.loss(preds, outputs)\n",
        "\n",
        "  # Test model\n",
        "  preds_test = mlp_on_gpu_cust.forward(X_test)\n",
        "  preds_test = tf.nn.softmax(preds_test)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_test_acc = accuracy.numpy()\n",
        "  inference_mnist_cust.append(np.sum(test_loss_total.numpy()) / X_test.shape[0])\n",
        "  inference_mnist_cust_acc.append(cur_test_acc)\n",
        "  print('Test loss: {:.4f} - Test Accuracy:{:.2f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0],cur_test_acc))"
      ],
      "metadata": {
        "id": "6pxm_MybL_xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot learning curve (with errors)\n",
        "\n",
        "errors= np.squeeze(inference_mnist)\n",
        "errors_opt = np.squeeze(inference_mnist_cust)\n",
        "plt.plot(errors,label ='Test Fashion  MNIST')\n",
        "plt.plot(errors_opt,label ='Test Fashion  MNIST custom optimization')\n",
        "plt.ylabel('errors')\n",
        "plt.xlabel('iterations ')\n",
        "plt.title(\"Fashion MNIST normal vs custom optimization\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r7ATpiTl85da"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "IST597_MLP_tfv5097_assignment03_fmnist.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}