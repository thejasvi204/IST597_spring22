{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##* Design MLP with 2 hidden layers (Input Layer - 2 hidden Layer - Output layer) to classify objects (fashionMNIST) and digits (MNIST).\n",
        "##* Design regularization approaches, and analyze drop/boost in performance of your model. Report results with atleast 2 regularization variants (droput, L1 penalty, L2, L1+L2, Normalization, Noise)\n",
        "\n",
        "#* tfv5097@psu.edu :Thejasvi Velaga"
      ],
      "metadata": {
        "id": "kkfX8FIJOHgb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPN_DtXq9mns",
        "outputId": "39f21416-4981-4f14-ffd7-4ef46d3de22a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ],
      "source": [
        "# baseline cnn model for mnist\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "seed=757493755\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vtpOvonMQj0f"
      },
      "outputs": [],
      "source": [
        "# Constants \n",
        "size_input = 784\n",
        "size_hidden = [128,64]\n",
        "size_output = 10\n",
        "number_of_train_examples = 7000 #60000\n",
        "number_of_test_examples = 1500   #10000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset Function"
      ],
      "metadata": {
        "id": "bxKhx0c7PK2r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "D5VqKmW_Sdhg"
      },
      "outputs": [],
      "source": [
        "#To load the data set for mist and fashion mnist\n",
        "def load_dataset():\n",
        "\t# load dataset for mnist\n",
        "\t(trainX_mnist, trainY_mnist), (testX_mnist, testY_mnist) = mnist.load_data()\n",
        " #load dataset for fashion mnist\n",
        "\t(trainX_fmnist, trainY_fmnist), (testX_fmnist, testY_fmnist) = fashion_mnist.load_data()\n",
        "\n",
        "\t# one hot encode target values\n",
        "\ttrainY_mnist = to_categorical(trainY_mnist)\n",
        "\ttestY_mnist = to_categorical(testY_mnist)\n",
        " \n",
        "\ttrainY_fmnist = to_categorical(trainY_fmnist)\n",
        "\ttestY_fmnist = to_categorical(testY_fmnist)\n",
        " \n",
        "\treturn trainX_mnist, trainY_mnist, testX_mnist, testY_mnist,trainX_fmnist, trainY_fmnist, testX_fmnist, testY_fmnist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Flatten,Normalize and Reduce data sets size function"
      ],
      "metadata": {
        "id": "dkwjTGt6PavV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B2tvSWaK3uFO"
      },
      "outputs": [],
      "source": [
        "# flatten\n",
        "def flat(train_mnist,test_mnist,train_fmnist,test_fmnist):\n",
        "  train_x_flatten_mnist = train_mnist.reshape(train_mnist.shape[0],-1)#.T\n",
        "  test_x_flatten_mnist = test_mnist.reshape(test_mnist.shape[0],-1)#.T\n",
        "  train_x_flatten_fmnist=train_fmnist.reshape(train_fmnist.shape[0],-1)#.T\n",
        "  test_x_flatten_fmnist=test_fmnist.reshape(test_fmnist.shape[0],-1)#.T\n",
        "  return train_x_flatten_mnist, test_x_flatten_mnist,train_x_flatten_fmnist, test_x_flatten_fmnist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jdfYTUmC3w8U"
      },
      "outputs": [],
      "source": [
        "# scale pixels\n",
        "def normalize(train_mnist, test_mnist,train_fmnist,test_fmnist):\n",
        "\t# convert from integers to floats\n",
        "  train_norm_mnist = train_mnist.astype('float32')\n",
        "  test_norm_mnist = test_mnist.astype('float32')\n",
        "  \n",
        "  train_norm_fmnist = train_fmnist.astype('float32')\n",
        "  test_norm_fmnist = test_fmnist.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "  train_norm_mnist = train_norm_mnist / 255.0\n",
        "  test_norm_mnist = test_norm_mnist / 255.0\n",
        "\n",
        "  train_norm_fmnist = train_norm_fmnist / 255.0\n",
        "  test_norm_fmnist = test_norm_fmnist / 255.0\n",
        "\t# return normalized images\n",
        "  return train_norm_mnist, test_norm_mnist,train_norm_fmnist, test_norm_fmnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "L7QC03soMmLz"
      },
      "outputs": [],
      "source": [
        "def reducedatasets(trainX_mnist,trainY_mnist,testX_mnist,testY_mnist,trainX_fmnist,trainY_fmnist,testX_fmnist,testY_fmnist):\n",
        "  train_x_mnist = trainX_mnist[:number_of_train_examples, :]\n",
        "  test_x_mnist = testX_mnist[:number_of_test_examples, :]\n",
        "  train_y_mnist = trainY_mnist[:number_of_train_examples, :]\n",
        "  test_y_mnist = testY_mnist[:number_of_test_examples, :]\n",
        "  train_x_fmnist = trainX_fmnist[:number_of_train_examples, :]\n",
        "  test_x_fmnist = testX_fmnist[:number_of_test_examples, :]\n",
        "  train_y_fmnist = trainY_fmnist[:number_of_train_examples, :]\n",
        "  test_y_fmnist = testY_fmnist[:number_of_test_examples, :]\n",
        "  return train_x_mnist,train_y_mnist, test_x_mnist,test_y_mnist,train_x_fmnist,train_y_fmnist, test_x_fmnist,test_y_fmnist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load,FLatten and Normalize "
      ],
      "metadata": {
        "id": "N9dbbDSmPtmx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-WKdR3GASjiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b88110f-15f9-4f7c-bddd-a7abed45eca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 1s 0us/step\n",
            "26435584/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# loading mnist and fashion mnist and normalizing \n",
        "X_train_mnist, y_train_mnist, X_test_mnist, y_test_mnist,X_train_fmnist, y_train_fmnist, X_test_fmnist, y_test_fmnist = load_dataset()\n",
        "X_train_mnist, X_test_mnist,X_train_fmnist, X_test_fmnist = flat(X_train_mnist, X_test_mnist,X_train_fmnist, X_test_fmnist)\n",
        "X_train_mnist, X_test_mnist,X_train_fmnist, X_test_fmnist = normalize(X_train_mnist, X_test_mnist,X_train_fmnist, X_test_fmnist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0J09SHcCMkX0"
      },
      "outputs": [],
      "source": [
        "# reducing training and testing sets \n",
        "X_train_mnist, y_train_mnist, X_test_mnist, y_test_mnist,X_train_fmnist, y_train_fmnist, X_test_fmnist, y_test_fmnist = reducedatasets(X_train_mnist, y_train_mnist, X_test_mnist, y_test_mnist,X_train_fmnist, y_train_fmnist, X_test_fmnist, y_test_fmnist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jUqjbthWQpcn"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches for mnist and fashion mnist\n",
        "train_ds_mnist = tf.data.Dataset.from_tensor_slices((X_train_mnist, y_train_mnist)).batch(32)\n",
        "test_ds_mnist = tf.data.Dataset.from_tensor_slices((X_test_mnist, y_test_mnist)).batch(4)\n",
        "\n",
        "train_ds_fmnist = tf.data.Dataset.from_tensor_slices((X_train_fmnist, y_train_fmnist)).batch(32)\n",
        "test_ds_fmnist = tf.data.Dataset.from_tensor_slices((X_test_fmnist, y_test_fmnist)).batch(4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eTNdGfElF3yt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9eb80bf-6a88-4135-9dcb-f0c297aad77d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " MNIST Dataset Shape:\n",
            "X_train-ds: <BatchDataset shapes: ((None, 784), (None, 10)), types: (tf.float32, tf.float32)>\n",
            "Y_train-ds: <BatchDataset shapes: ((None, 784), (None, 10)), types: (tf.float32, tf.float32)>\n",
            " fashion MNIST Dataset Shape:\n",
            "X_train-ds: <BatchDataset shapes: ((None, 784), (None, 10)), types: (tf.float32, tf.float32)>\n",
            "Y_train-ds: <BatchDataset shapes: ((None, 784), (None, 10)), types: (tf.float32, tf.float32)>\n"
          ]
        }
      ],
      "source": [
        "#Testing the mnist and fashion mnist loading\n",
        "print(' MNIST Dataset Shape:')\n",
        "print('X_train-ds: ' + str(train_ds_mnist))\n",
        "print('Y_train-ds: ' + str(test_ds_mnist))\n",
        "print(' fashion MNIST Dataset Shape:')\n",
        "print('X_train-ds: ' + str(train_ds_fmnist))\n",
        "print('Y_train-ds: ' + str(test_ds_fmnist))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_70kdmB6sVbH"
      },
      "source": [
        "##**The below 3 blocks codes is on models without reguralization,with drop out reguralization and with L1 reguralization repesctively.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nd7neF1BQpfu"
      },
      "outputs": [],
      "source": [
        "#without any regularization.\n",
        "class MLP(tf.keras.Model):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    super(MLP, self).__init__()\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "\n",
        "    # self.size_input = 784\n",
        "    # self.size_hidden = 128\n",
        "    # self.size_output = 10\n",
        "    # self.device = 'gpu'\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer and hideen layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "    # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    #self.MLP_variables = [self.W1, self.W2,self.W3,self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "    self.MLP_variables = [self.W1, self.W2,self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    categorical_cross_entropy = tf.keras.losses.CategoricalCrossentropy()\n",
        "    return categorical_cross_entropy(y_true=y_true,y_pred=y_pred)\n",
        "    #return tf.keras.losses.sparse_categorical_crossentropy(y_true,y_pred)\n",
        "    #return tf.losses.mean_squared_error(y_true_tf, y_pred_tf)\n",
        "    #return tf.nn.softmax_cross_entropy_with_logits(logits=y_pred,labels=y_true)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    #optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "    #optimizer = tf.keras.optimizers.Adamax(learning_rate=1e-5)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.MLP_variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.MLP_variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    X_tf=X_tf/255.0\n",
        "    # Compute values in hidden layer\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat = tf.nn.relu(what)\n",
        "\n",
        "\n",
        "    what1 = tf.matmul(hhat, self.W2) + self.b2\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat1, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    output=tf.nn.softmax(output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with drop out regularization.\n",
        "class MLP_Regdp(tf.keras.Model):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    super(MLP_Regdp, self).__init__()\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "\n",
        "    # self.size_input = 784\n",
        "    # self.size_hidden = 128\n",
        "    # self.size_output = 10\n",
        "    # self.device = 'gpu'\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer and hideen layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "    # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    #self.MLP_variables = [self.W1, self.W2,self.W3,self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "    self.MLP_variables = [self.W1, self.W2,self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    categorical_cross_entropy = tf.keras.losses.CategoricalCrossentropy()\n",
        "    return categorical_cross_entropy(y_true=y_true,y_pred=y_pred)\n",
        "    #return tf.keras.losses.sparse_categorical_crossentropy(y_true,y_pred)\n",
        "    #return tf.losses.mean_squared_error(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    #optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    #optimizer = tf.keras.optimizers.Adamax(learning_rate=1e-5)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.MLP_variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.MLP_variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    X_tf=X_tf/255.0\n",
        "    # Compute values in hidden layer\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat = tf.nn.relu(what)\n",
        "    drop_out = tf.nn.dropout(hhat, 0.2)\n",
        "    what1 = tf.matmul(drop_out, self.W2) + self.b2\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    drop_out1 = tf.nn.dropout(hhat1, 0.2)\n",
        "    # Compute output\n",
        "    #adding dropout\n",
        "    output = tf.matmul(drop_out1, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    output=tf.nn.softmax(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "eJvPuViuEYVT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with L2  regularization.\n",
        "class MLP_RegL(tf.keras.Model):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device=None):\n",
        "    super(MLP_RegL, self).__init__()\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "\n",
        "    # self.size_input = 784\n",
        "    # self.size_hidden = 128\n",
        "    # self.size_output = 10\n",
        "    # self.device = 'gpu'\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "    \n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden[0]]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden[0]]))\n",
        "\n",
        "    # Initialize weights between hidden layer and hideen layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden[0], self.size_hidden[1]]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden[1]]))\n",
        "\n",
        "    # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden[1], self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    #self.MLP_variables = [self.W1, self.W2,self.W3,self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "    self.MLP_variables = [self.W1, self.W2,self.W3, self.b1, self.b2, self.b3]\n",
        "    \n",
        "    \n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    categorical_cross_entropy = tf.keras.losses.CategoricalCrossentropy()\n",
        "    return categorical_cross_entropy(y_true=y_true,y_pred=y_pred) +0.01*tf.nn.l2_loss(self.W2) +0.01*tf.nn.l2_loss(self.W3)\n",
        "    #return tf.keras.losses.sparse_categorical_crossentropy(y_true,y_pred)\n",
        "    #return tf.losses.mean_squared_error(y_true_tf, y_pred_tf)\n",
        "  \n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    #optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    #optimizer = tf.keras.optimizers.Adamax(learning_rate=1e-5)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.MLP_variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.MLP_variables))\n",
        "        \n",
        "        \n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    X_tf=X_tf/255.0\n",
        "    # Compute values in hidden layer\n",
        "    what = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat = tf.nn.relu(what)\n",
        "\n",
        "\n",
        "    what1 = tf.matmul(hhat, self.W2) + self.b2\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat1, self.W3) + self.b3\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    output=tf.nn.softmax(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "Qj-G4mz_FXhC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "n9w8SJ9Xf-qT"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10\n",
        "iterations=[1,2,3,4,5,6,7,8,9,10]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_accuracy( y_pred,y_true):\n",
        "    return tf.cast(tf.equal(tf.argmax(y_true, axis=-1),tf.argmax(y_pred, axis=-1)),tf.keras.backend.floatx())"
      ],
      "metadata": {
        "id": "45Tad5gzYBcL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRCVni6FsF3j"
      },
      "source": [
        "##**The below 3 blocks codes is on MNIST without reguralization,with drop out reguralization and with L1 reguralization repesctively.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih59_ZCl9D07",
        "outputId": "b78319db-0898-440d-de98-34c886825483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For MNIST GPU without Reguralization\n",
            "\n",
            "Number of Epoch = 1 - Average categorical_cross_entropy:= 0.45267368861607143 - accuracy:=99.875\n",
            "Number of Epoch = 2 - Average categorical_cross_entropy:= 0.4526974051339286 - accuracy:=99.91666666666667\n",
            "Number of Epoch = 3 - Average categorical_cross_entropy:= 0.4526493791852679 - accuracy:=99.83333333333333\n",
            "Number of Epoch = 4 - Average categorical_cross_entropy:= 0.4527213309151786 - accuracy:=99.95833333333333\n",
            "Number of Epoch = 5 - Average categorical_cross_entropy:= 0.4526734793526786 - accuracy:=99.875\n",
            "Number of Epoch = 6 - Average categorical_cross_entropy:= 0.45267344447544644 - accuracy:=99.875\n",
            "Number of Epoch = 7 - Average categorical_cross_entropy:= 0.45267344447544644 - accuracy:=99.875\n",
            "Number of Epoch = 8 - Average categorical_cross_entropy:= 0.45269737025669643 - accuracy:=99.91666666666667\n",
            "Number of Epoch = 9 - Average categorical_cross_entropy:= 0.45269747488839285 - accuracy:=99.91666666666667\n",
            "Number of Epoch = 10 - Average categorical_cross_entropy:= 0.45267344447544644 - accuracy:=99.875\n",
            "\n",
            "Total time taken (in seconds): 84.05\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU for mnist with out regularization train set\n",
        "#mlp_on_gpu = MLP()\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "time_start = time.time()\n",
        "acce_mnist_train=[]\n",
        "#acce_mnist_test=[]\n",
        "print(\"For MNIST GPU without Reguralization\\n\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy=0\n",
        "  train_ds_mnist = tf.data.Dataset.from_tensor_slices((X_train_mnist, y_train_mnist)).shuffle(25, seed=epoch*(seed)).batch(32)\n",
        "  for inputs, outputs in train_ds_mnist:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "    accuracy=categorical_accuracy(preds, outputs)\n",
        "  acce_mnist_train.append(np.sum(loss_total_gpu) / X_train_mnist.shape[0])\n",
        "  print('Number of Epoch = {} - Average categorical_cross_entropy:= {} - accuracy:={}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train_mnist.shape[0],(100-(np.abs(np.sum(np.abs(accuracy)))/accuracy.shape[0]))))\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Ro0m8_Nhfv_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2c4a1b4-0006-4bab-dbb5-8a7d68dcd71d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For MNIST GPU with Reguralization Drop Out\n",
            "\n",
            "Number of Epoch = 1 - Average categorical_cross_entropy:= 0.5822149135044643 - accuracy:=99.9375\n",
            "Number of Epoch = 2 - Average categorical_cross_entropy:= 0.5808330775669643 - accuracy:=99.875\n",
            "Number of Epoch = 3 - Average categorical_cross_entropy:= 0.5823743722098215 - accuracy:=99.9375\n",
            "Number of Epoch = 4 - Average categorical_cross_entropy:= 0.581331787109375 - accuracy:=99.875\n",
            "Number of Epoch = 5 - Average categorical_cross_entropy:= 0.5802337820870536 - accuracy:=99.9375\n",
            "Number of Epoch = 6 - Average categorical_cross_entropy:= 0.5778423549107143 - accuracy:=99.9375\n",
            "Number of Epoch = 7 - Average categorical_cross_entropy:= 0.58142578125 - accuracy:=100.0\n",
            "Number of Epoch = 8 - Average categorical_cross_entropy:= 0.5737910505022321 - accuracy:=99.8125\n",
            "Number of Epoch = 9 - Average categorical_cross_entropy:= 0.5828118373325892 - accuracy:=99.9375\n",
            "Number of Epoch = 10 - Average categorical_cross_entropy:= 0.5757952008928572 - accuracy:=99.9375\n",
            "\n",
            "Total time taken (in seconds): 126.99\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU for mnist with  regularization drop out\n",
        "#mlp_on_gpu = MLP()\n",
        "mlp_on_gpu = MLP_Regdp(size_input, size_hidden, size_output, device='cpu')\n",
        "time_start = time.time()\n",
        "acce_mnist_train_rdp=[]\n",
        "print(\"For MNIST GPU with Reguralization Drop Out\\n\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy=0\n",
        "  train_ds_mnist = tf.data.Dataset.from_tensor_slices((X_train_mnist, y_train_mnist)).shuffle(25, seed=epoch*(seed)).batch(24)\n",
        "  for inputs, outputs in train_ds_mnist:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "    accuracy=categorical_accuracy(preds, outputs)\n",
        "  acce_mnist_train_rdp.append(np.sum(loss_total_gpu) / X_train_mnist.shape[0])\n",
        "  print('Number of Epoch = {} - Average categorical_cross_entropy:= {} - accuracy:={}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train_mnist.shape[0],(100-(np.abs(np.sum(np.abs(accuracy)))/accuracy.shape[0]))))\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "ffJW_D6arbXZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4097db51-4ce3-419c-9a25-7535a9c9221a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For MNIST GPU with Reguralization L2\n",
            "\n",
            "Number of Epoch = 1 - Average categorical_cross_entropy:= 1.6820062779017857 - accuracy:=99.75\n",
            "Number of Epoch = 2 - Average categorical_cross_entropy:= 1.6255678013392858 - accuracy:=99.875\n",
            "Number of Epoch = 3 - Average categorical_cross_entropy:= 1.5853204520089286 - accuracy:=99.83333333333333\n",
            "Number of Epoch = 4 - Average categorical_cross_entropy:= 1.546716517857143 - accuracy:=99.66666666666667\n",
            "Number of Epoch = 5 - Average categorical_cross_entropy:= 1.5093159877232143 - accuracy:=99.58333333333333\n",
            "Number of Epoch = 6 - Average categorical_cross_entropy:= 1.4731943359375 - accuracy:=99.70833333333333\n",
            "Number of Epoch = 7 - Average categorical_cross_entropy:= 1.4067544642857144 - accuracy:=99.70833333333333\n",
            "Number of Epoch = 8 - Average categorical_cross_entropy:= 1.358173828125 - accuracy:=99.70833333333333\n",
            "Number of Epoch = 9 - Average categorical_cross_entropy:= 1.325374302455357 - accuracy:=99.5\n",
            "Number of Epoch = 10 - Average categorical_cross_entropy:= 1.2933842075892856 - accuracy:=99.45833333333333\n",
            "\n",
            "Total time taken (in seconds): 97.17\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU for mnist with  regularization L2 train set\n",
        "#mlp_on_gpu = MLP()\n",
        "mlp_on_gpu = MLP_RegL(size_input, size_hidden, size_output, device='cpu')\n",
        "time_start = time.time()\n",
        "acce_mnist_train_rl=[]\n",
        "print(\"For MNIST GPU with Reguralization L2\\n\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy=0\n",
        "  train_ds_mnist = tf.data.Dataset.from_tensor_slices((X_train_mnist, y_train_mnist)).shuffle(25, seed=epoch*(seed)).batch(32)\n",
        "  for inputs, outputs in train_ds_mnist:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "    accuracy=categorical_accuracy(preds, outputs)\n",
        "  acce_mnist_train_rl.append(np.sum(loss_total_gpu) / X_train_mnist.shape[0])\n",
        "  print('Number of Epoch = {} - Average categorical_cross_entropy:= {} - accuracy:={}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train_mnist.shape[0],(100-(np.abs(np.sum(np.abs(accuracy)))/accuracy.shape[0]))))\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnVFo00CrlMV"
      },
      "source": [
        "##**The below 3 blocks codes is on fashion MNIST without reguralization,with drop out reguralization and with L1 reguralization repesctively.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGG7huxA9HQU",
        "outputId": "81bd527f-1a9f-4156-949e-614194291df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Fashion MNIST GPU without Reguralization\n",
            "\n",
            "Number of Epoch = 1 - Average categorical_cross_entropy:= 0.4442196568080357 - accuracy:=99.91666666666667\n",
            "Number of Epoch = 2 - Average categorical_cross_entropy:= 0.4249189801897321 - accuracy:=99.91666666666667\n",
            "Number of Epoch = 3 - Average categorical_cross_entropy:= 0.4126382533482143 - accuracy:=99.91666666666667\n",
            "Number of Epoch = 4 - Average categorical_cross_entropy:= 0.4111673060825893 - accuracy:=99.875\n",
            "Number of Epoch = 5 - Average categorical_cross_entropy:= 0.410490234375 - accuracy:=99.83333333333333\n",
            "Number of Epoch = 6 - Average categorical_cross_entropy:= 0.4099150390625 - accuracy:=99.875\n",
            "Number of Epoch = 7 - Average categorical_cross_entropy:= 0.40936146763392856 - accuracy:=99.79166666666667\n",
            "Number of Epoch = 8 - Average categorical_cross_entropy:= 0.4089474051339286 - accuracy:=99.91666666666667\n",
            "Number of Epoch = 9 - Average categorical_cross_entropy:= 0.4084927455357143 - accuracy:=99.875\n",
            "Number of Epoch = 10 - Average categorical_cross_entropy:= 0.4080311802455357 - accuracy:=99.70833333333333\n",
            "\n",
            "Total time taken (in seconds): 113.01\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU for fashion mnist without reguralization\n",
        "#mlp_on_gpu = MLP()\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='gpu')\n",
        "time_start = time.time()\n",
        "print(\"For Fashion MNIST GPU without Reguralization\\n\")\n",
        "acce_fmnist_train=[]\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy=0\n",
        "  train_ds_fmnist = tf.data.Dataset.from_tensor_slices((X_train_fmnist, y_train_fmnist)).shuffle(25, seed=epoch*(1234)).batch(32)\n",
        "  for inputs, outputs in train_ds_fmnist:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "    accuracy=categorical_accuracy(preds, outputs)\n",
        "  acce_fmnist_train.append(np.sum(loss_total_gpu) / X_train_fmnist.shape[0])\n",
        "  print('Number of Epoch = {} - Average categorical_cross_entropy:= {} - accuracy:={}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train_fmnist.shape[0],(100-(np.abs(np.sum(np.abs(accuracy)))/accuracy.shape[0]))))\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "i6o4XZ3CqfK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a260cba1-1c4d-46cc-bba7-ed6df700ffc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Fashion MNIST GPU with Reguralization Drop Out\n",
            "\n",
            "Number of Epoch = 1 - Average categorical_cross_entropy:= 0.440268310546875 - accuracy:=99.91666666666667\n",
            "Number of Epoch = 2 - Average categorical_cross_entropy:= 0.44065237862723217 - accuracy:=99.875\n",
            "Number of Epoch = 3 - Average categorical_cross_entropy:= 0.4381923130580357 - accuracy:=99.91666666666667\n",
            "Number of Epoch = 4 - Average categorical_cross_entropy:= 0.4384303501674107 - accuracy:=99.875\n",
            "Number of Epoch = 5 - Average categorical_cross_entropy:= 0.4374399065290179 - accuracy:=99.79166666666667\n",
            "Number of Epoch = 6 - Average categorical_cross_entropy:= 0.4377294224330357 - accuracy:=99.875\n",
            "Number of Epoch = 7 - Average categorical_cross_entropy:= 0.4363187430245536 - accuracy:=99.875\n",
            "Number of Epoch = 8 - Average categorical_cross_entropy:= 0.43549072265625 - accuracy:=99.79166666666667\n",
            "Number of Epoch = 9 - Average categorical_cross_entropy:= 0.43242996651785715 - accuracy:=99.875\n",
            "Number of Epoch = 10 - Average categorical_cross_entropy:= 0.43418331473214283 - accuracy:=99.83333333333333\n",
            "\n",
            "Total time taken (in seconds): 89.09\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU for  fashion mnist with  regularization drop out\n",
        "#mlp_on_gpu = MLP()\n",
        "mlp_on_gpu = MLP_Regdp(size_input, size_hidden, size_output, device='gpu')\n",
        "time_start = time.time()\n",
        "acce_fmnist_train_rdp=[]\n",
        "print(\"For Fashion MNIST GPU with Reguralization Drop Out\\n\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy=0\n",
        "  train_ds_fmnist = tf.data.Dataset.from_tensor_slices((X_train_fmnist, y_train_fmnist)).shuffle(25, seed=epoch*(seed)).batch(32)\n",
        "  for inputs, outputs in train_ds_fmnist:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "    accuracy=categorical_accuracy(preds, outputs)\n",
        "  acce_fmnist_train_rdp.append(np.sum(loss_total_gpu) / X_train_fmnist.shape[0])\n",
        "  print('Number of Epoch = {} - Average categorical_cross_entropy:= {} - accuracy:={}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train_fmnist.shape[0],(100-(np.abs(np.sum(np.abs(accuracy)))/accuracy.shape[0]))))\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "xO9iOMm8qz1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0090b94-0868-4e90-dd08-aea93a0083df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Fashion MNIST GPU with Reguralization L2\n",
            "\n",
            "Number of Epoch = 1 - Average categorical_cross_entropy:= 1.8063604910714286 - accuracy:=99.875\n",
            "Number of Epoch = 2 - Average categorical_cross_entropy:= 1.7316092354910715 - accuracy:=99.83333333333333\n",
            "Number of Epoch = 3 - Average categorical_cross_entropy:= 1.6341934988839286 - accuracy:=99.75\n",
            "Number of Epoch = 4 - Average categorical_cross_entropy:= 1.594478515625 - accuracy:=99.75\n",
            "Number of Epoch = 5 - Average categorical_cross_entropy:= 1.5565185546875 - accuracy:=99.75\n",
            "Number of Epoch = 6 - Average categorical_cross_entropy:= 1.520040736607143 - accuracy:=99.79166666666667\n",
            "Number of Epoch = 7 - Average categorical_cross_entropy:= 1.4844732142857142 - accuracy:=99.83333333333333\n",
            "Number of Epoch = 8 - Average categorical_cross_entropy:= 1.4498067801339285 - accuracy:=99.75\n",
            "Number of Epoch = 9 - Average categorical_cross_entropy:= 1.4162222377232143 - accuracy:=99.75\n",
            "Number of Epoch = 10 - Average categorical_cross_entropy:= 1.383275111607143 - accuracy:=99.70833333333333\n",
            "\n",
            "Total time taken (in seconds): 84.02\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU for  fashion mnist with  regularization L2\n",
        "#mlp_on_gpu = MLP()\n",
        "mlp_on_gpu = MLP_RegL(size_input, size_hidden, size_output, device='gpu')\n",
        "time_start = time.time()\n",
        "print(\"For Fashion MNIST GPU with Reguralization L2\\n\")\n",
        "acce_fmnist_train_rl=[]\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy=0\n",
        "  train_ds_fmnist = tf.data.Dataset.from_tensor_slices((X_train_fmnist, y_train_fmnist)).shuffle(25, seed=epoch*(seed)).batch(32)\n",
        "  for inputs, outputs in train_ds_fmnist:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "    accuracy=categorical_accuracy(preds, outputs)\n",
        "  acce_fmnist_train_rl.append(np.sum(loss_total_gpu) / X_train_fmnist.shape[0])\n",
        "  print('Number of Epoch = {} - Average categorical_cross_entropy:= {} - accuracy:={}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train_fmnist.shape[0],(100-(np.abs(np.sum(np.abs(accuracy)))/accuracy.shape[0]))))\n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plotting for MNIST for 3 models"
      ],
      "metadata": {
        "id": "-Dmt9Qu3QI5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot learning curve (with errors)\n",
        "errors = np.squeeze(acce_mnist_train)\n",
        "errors_rdp = np.squeeze(acce_mnist_train_rdp)\n",
        "errors_rl = np.squeeze(acce_mnist_train_rl)\n",
        "plt.plot(iterations,errors,label ='MNIST')\n",
        "plt.plot(iterations,errors_rdp,label ='MNIST drop out')\n",
        "plt.plot(iterations,errors_rl,label ='MNIST L2')\n",
        "plt.ylabel('errors')\n",
        "plt.xlabel('iterations ')\n",
        "plt.title(\"MNIST\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "r3aM-WJbQMD0",
        "outputId": "e96b817e-3b15-4d0e-a60c-79c83257885b"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plotting for fashion MNIST for 3 models"
      ],
      "metadata": {
        "id": "OuZRkLeKaMNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot learning curve (with errors)\n",
        "errors = np.squeeze(acce_fmnist_train)\n",
        "errors_rdp = np.squeeze(acce_fmnist_train_rdp)\n",
        "errors_rl = np.squeeze(acce_fmnist_train_rl)\n",
        "plt.plot(iterations,errors,label='Fashion MNIST')\n",
        "plt.plot(iterations,errors_rdp,label='Fashion MNIST drop out')\n",
        "plt.plot(iterations,errors_rl,label='Fashion MNIST L2')\n",
        "plt.ylabel('errors')\n",
        "plt.xlabel('iterations ')\n",
        "plt.title(\"Fashion MNIST\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "-uwKg3QDaD2y",
        "outputId": "c66ce389-3c85-4e35-c2ae-cc60bd17634e"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiORnznaqvsU"
      },
      "source": [
        "## One Step Inference\n",
        "## Default Mode for MNIST"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "#Default mode for L2 reguralization\n",
        "mlp_on_default_l = MLP_RegL(size_input, size_hidden, size_output)\n",
        "#Default mode for drop out\n",
        "mlp_on_default_dp = MLP_Regdp(size_input, size_hidden, size_output)\n"
      ],
      "metadata": {
        "id": "v50dIkY6RaNt"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_mnist = tf.data.Dataset.from_tensor_slices((X_train_mnist, y_train_mnist)).shuffle(25, seed=epoch*(seed)).batch(32)"
      ],
      "metadata": {
        "id": "KwzW_lbsZSDY"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "3bYYBYs0k9Mt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168c5ff8-40f6-4296-b131-6f0d4760810d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For MNIST inference\n",
            "\n",
            "Test categorical_cross_entropy: 0.4542- Accuracy:=99.95833333333333\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "accuracy=0\n",
        "for inputs, outputs in train_ds_mnist:\n",
        "  preds = mlp_on_default.forward(inputs)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_default.loss(preds, outputs)\n",
        "  accuracy=categorical_accuracy(preds, outputs)\n",
        "# a = (test_loss_total.numpy() / X_train.shape[0])\n",
        "# print(X_train.shape[0])\n",
        "# print(test_loss_total.numpy())\n",
        "# print(b)\n",
        "print(\"For MNIST inference\\n\")\n",
        "print('Test categorical_cross_entropy: {:.4f}- Accuracy:={}'.format(np.sum(test_loss_total.numpy()) / X_train_mnist.shape[0],(100-(np.abs(np.sum(np.abs(accuracy)))/accuracy.shape[0]))))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "accuracy=0\n",
        "for inputs, outputs in train_ds_mnist:\n",
        "  preds = mlp_on_default_dp.forward(inputs)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_default_dp.loss(preds, outputs)\n",
        "  accuracy=categorical_accuracy(preds, outputs)\n",
        "# a = (test_loss_total.numpy() / X_train.shape[0])\n",
        "# print(X_train.shape[0])\n",
        "# print(test_loss_total.numpy())\n",
        "# print(b)\n",
        "print(\"For MNIST inference\\n\")\n",
        "print('Test categorical_cross_entropy: {:.4f}- Accuracy:={}'.format(np.sum(test_loss_total.numpy()) / X_train_mnist.shape[0],(100-(np.abs(np.sum(np.abs(accuracy)))/accuracy.shape[0]))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO3jNlxtStwV",
        "outputId": "0ff0e3fc-f09b-4b5e-a596-79f03969ed7f"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For MNIST inference\n",
            "\n",
            "Test categorical_cross_entropy: 0.4522- Accuracy:=99.875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "accuracy=0\n",
        "for inputs, outputs in train_ds_mnist:\n",
        "  preds = mlp_on_default_l.forward(inputs)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_default_l.loss(preds, outputs)\n",
        "  accuracy=categorical_accuracy(preds, outputs)\n",
        "# a = (test_loss_total.numpy() / X_train.shape[0])\n",
        "# print(X_train.shape[0])\n",
        "# print(test_loss_total.numpy())\n",
        "# print(b)\n",
        "print(\"For MNIST inference\\n\")\n",
        "print('Test categorical_cross_entropy: {:.4f}- Accuracy:={}'.format(np.sum(test_loss_total.numpy()) / X_train_mnist.shape[0],(100-(np.abs(np.sum(np.abs(accuracy)))/accuracy.shape[0]))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUhT20foSu3T",
        "outputId": "e2ac6222-6b83-4ee5-a107-3db4a77089bc"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For MNIST inference\n",
            "\n",
            "Test categorical_cross_entropy: 1.8758- Accuracy:=99.95833333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Default Mode for Fashion MNIST"
      ],
      "metadata": {
        "id": "BEeLLHczS_Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Default mode\n",
        "mlp_on_default = MLP(size_input, size_hidden, size_output)\n",
        "\n",
        "#Default mode for drop out\n",
        "mlp_on_default_dp = MLP_Regdp(size_input, size_hidden, size_output)\n",
        "\n",
        "#Default mode for L2 reguralization\n",
        "mlp_on_default_l = MLP_RegL(size_input, size_hidden, size_output)"
      ],
      "metadata": {
        "id": "baOLeT2VSOkJ"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_fmnist = tf.data.Dataset.from_tensor_slices((X_train_fmnist, y_train_fmnist)).shuffle(25, seed=epoch*(seed)).batch(32)"
      ],
      "metadata": {
        "id": "dYejf7zHaCDm"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "jjnpqXFUmETY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867bad48-4bae-438a-c8a5-28f34ece1504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Fashion MNIST inference\n",
            "\n",
            "Test categorical_cross_entropy: 0.4545- Accuracy:=99.875\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "accuracy=0\n",
        "for inputs, outputs in train_ds_fmnist:\n",
        "  preds = mlp_on_default.forward(inputs)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_default.loss(preds, outputs)\n",
        "  accuracy=categorical_accuracy(preds, outputs)\n",
        "# a = (test_loss_total.numpy() / X_train.shape[0])\n",
        "# print(X_train.shape[0])\n",
        "# print(test_loss_total.numpy())\n",
        "# print(b)\n",
        "print(\"For Fashion MNIST inference\\n\")\n",
        "print('Test categorical_cross_entropy: {:.4f}- Accuracy:={}'.format(np.sum(test_loss_total.numpy()) / X_train_fmnist.shape[0],(100-(np.abs(np.sum(np.abs(accuracy)))/accuracy.shape[0]))))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "accuracy=0\n",
        "for inputs, outputs in train_ds_fmnist:\n",
        "  preds = mlp_on_default_dp.forward(inputs)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_default_dp.loss(preds, outputs)\n",
        "  accuracy=categorical_accuracy(preds, outputs)\n",
        "# a = (test_loss_total.numpy() / X_train.shape[0])\n",
        "# print(X_train.shape[0])\n",
        "# print(test_loss_total.numpy())\n",
        "# print(b)\n",
        "print(\"For Fashion MNIST inference\\n\")\n",
        "print('Test categorical_cross_entropy: {:.4f}- Accuracy:={}'.format(np.sum(test_loss_total.numpy()) / X_train_fmnist.shape[0],(100-(np.abs(np.sum(np.abs(accuracy)))/accuracy.shape[0]))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h8Ze3LFUNHm",
        "outputId": "ee1ebfae-e2fb-4f8d-d584-6a2af8dd8770"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Fashion MNIST inference\n",
            "\n",
            "Test categorical_cross_entropy: 0.4254- Accuracy:=99.95833333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "accuracy=0\n",
        "for inputs, outputs in train_ds_fmnist:\n",
        "  preds = mlp_on_default_l.forward(inputs)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_default_l.loss(preds, outputs)\n",
        "  accuracy=categorical_accuracy(preds, outputs)\n",
        "# a = (test_loss_total.numpy() / X_train.shape[0])\n",
        "# print(X_train.shape[0])\n",
        "# print(test_loss_total.numpy())\n",
        "# print(b)\n",
        "print(\"For Fashion MNIST inference\\n\")\n",
        "print('Test categorical_cross_entropy: {:.4f}- Accuracy:={}'.format(np.sum(test_loss_total.numpy()) / X_train_fmnist.shape[0],(100-(np.abs(np.sum(np.abs(accuracy)))/accuracy.shape[0]))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbbs1S3dUNUY",
        "outputId": "267acad6-2b66-4121-bed7-d0f7f6e5bcdd"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Fashion MNIST inference\n",
            "\n",
            "Test categorical_cross_entropy: 1.8102- Accuracy:=99.95833333333333\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "IST597_MLP_tfv5097_assignment02.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}